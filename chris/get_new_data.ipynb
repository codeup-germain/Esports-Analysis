{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06840500",
   "metadata": {},
   "source": [
    "# Getting New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb4ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import prepare\n",
    "from time import sleep\n",
    "from env import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03144a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Josh's code\n",
    "basegame = 4108071572\n",
    "url = 'https://americas.api.riotgames.com/'\n",
    "name = 'NoNoYogi'\n",
    "response = requests.get(f\"https://na1.api.riotgames.com/lol/summoner/v4/summoners/by-name/{name}?api_key={api_key}\")\n",
    "puuid =response.json()['puuid']\n",
    "pulls = 0\n",
    "count = 1\n",
    "while pulls != 100:\n",
    "    \n",
    "    query = f'lol/match/v5/matches/by-puuid/{puuid}/ids?start={count}&count=1&api_key={api_key}'\n",
    "    response = requests.get(url+query)\n",
    "    \n",
    "    for game in response.json():\n",
    "        if int(game[4:]) > basegame:\n",
    "            print(\"higher\")\n",
    "        else:\n",
    "            print('lower')\n",
    "    count +=1\n",
    "    pulls +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jared's code\n",
    "def get_new_match_ids(puuids):\n",
    "    url = 'https://americas.api.riotgames.com/'\n",
    "    for puuid in puuids:\n",
    "        i = 1\n",
    "        while i < 20:\n",
    "            query = f'lol/match/v5/matches/by-puuid/{puuid}/ids?start={i-1}&count={i}&api_key={api_key}'\n",
    "            response = requests.get(url+query)\n",
    "            match = response.json()\n",
    "            match_num = int(match[0].strip('NA1_'))\n",
    "            if match_num > 4108071572:\n",
    "                i += 1\n",
    "                match_list.append(match)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420dfe2d",
   "metadata": {},
   "source": [
    "### Combine the Code Above Into One Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a45614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_patch_data(name_list, api_key):\n",
    "    \"\"\"\n",
    "    This function takes in a list of usernames that are in the same tier and iterates \n",
    "    through them to find their puuids.\n",
    "    It will then use those puuids to grab all of their match ids that are in the newest patch.\n",
    "    From there, the function will iterate through all of the match ids and grab the timeline\n",
    "    data for each match as well as the other game data. \n",
    "    Finally, this function will incrementally save the returned json information every 10 times.\n",
    "    This prevents the loss of all data if the function errors out at some point.\n",
    "    This function returns nothing.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Set up the basegame number. Any match_ids below this number are part of the old patch\n",
    "    #and will not be included. We will use the basegame var for comparisons in the loop\n",
    "    basegame = 4107000001\n",
    "    \n",
    "    #Set up the base url for grabbing match_ids\n",
    "    match_url = 'https://americas.api.riotgames.com/'\n",
    "    \n",
    "    #Set up the base url for grabbing puuids\n",
    "    puuid_base_url = 'https://na1.api.riotgames.com/'\n",
    "    \n",
    "    #Set up another counter to keep track of how many users have been completed\n",
    "    n = 0\n",
    "    \n",
    "    #Set up a counter to keep track of how many json files have been completed in total\n",
    "    num_json = 0\n",
    "    \n",
    "    #Set up one more counter to keep track of how many pulls we have made.\n",
    "    #Can't go over 100 pulls every 2 minutes.\n",
    "    num_pulls = 0\n",
    "    \n",
    "    #Set up empty lists to store timeline and other game data\n",
    "    timeline_data = []\n",
    "    game_data = []\n",
    "    \n",
    "    #Loop through each name in the given name_list\n",
    "    #First grab the puuid, then grab the match_ids, then actual match json info\n",
    "    for name in name_list:\n",
    "        \n",
    "        #for testing\n",
    "        print(f'Beginning Loop for user {name}')\n",
    "        \n",
    "        #Update n var\n",
    "        n += 1\n",
    "        \n",
    "        #Leave a status update after every 100 users\n",
    "        if n % 100 == 0:\n",
    "            print(f'Completed {n} users of {len(name_list)}!\\n')\n",
    "        \n",
    "        ########################## Get puuids ##########################\n",
    "        \n",
    "        #Set up the query\n",
    "        query = f'lol/summoner/v4/summoners/by-name/{name}?api_key={api_key}'\n",
    "        \n",
    "        #Get the response\n",
    "        response = requests.get(puuid_base_url + query)\n",
    "        \n",
    "        #Update pull count and sleep for 150 seconds if necessary\n",
    "        num_pulls += 1\n",
    "        \n",
    "        if num_pulls % 100 == 0:\n",
    "            print('\\nHit 100 pulls! Sleeping for 150 seconds.')\n",
    "            sleep(150)\n",
    "            print('Continuing!\\n')\n",
    "        \n",
    "        #Check the response code and leave message if something goes wrong\n",
    "        if response.status_code != 200:\n",
    "            print(f'Something went wrong! Status Code: {response.status_code}. Did not get puuid for user: {name}')\n",
    "            print(f'Skipping this user. There are {len(name_list) - n} users left.\\n')\n",
    "            continue\n",
    "        \n",
    "        #Grab the user's puuid\n",
    "        puuid = response.json()['puuid']\n",
    "        \n",
    "        #For testing\n",
    "        print(f'\\t-Got the puuid for user {name}.')\n",
    "        \n",
    "        ########################## Get Match IDs #########################\n",
    "        \n",
    "        #Now that we have a single puuid, grab all the match ids for that user that are greater\n",
    "        #than the basegame value. These match_ids will be for the newest patch.\n",
    "        \n",
    "        #Set up a counter\n",
    "        i = 0\n",
    "        \n",
    "        #This loop will be set up to run indefinitely. Be sure to break out of it when \n",
    "        #Done with the current user\n",
    "        while i >= 0:\n",
    "            #Set up the query to get a match_id\n",
    "            query = f'lol/match/v5/matches/by-puuid/{puuid}/ids?start={i}&count=1&api_key={api_key}'\n",
    "            \n",
    "            #Get the response\n",
    "            response = requests.get(match_url + query)\n",
    "            \n",
    "            #Update pull count and sleep for 150 seconds if necessary\n",
    "            num_pulls += 1\n",
    "\n",
    "            if num_pulls % 100 == 0:\n",
    "                print('\\nHit 100 pulls! Sleeping for 150 seconds.')\n",
    "                sleep(150)\n",
    "                print('Continuing!\\n')\n",
    "            \n",
    "            #Check response status code and leave message if something went wrong\n",
    "            if response.status_code != 200:\n",
    "                print(f'Something went wrong! Status Code: {response.status_code}. Did not get Match_ID for user: {name}')\n",
    "                print(f'Skipping this user. There are {len(name_list) - n} users left.\\n')\n",
    "                break\n",
    "                \n",
    "            #Grab the match_id\n",
    "            match = response.json()\n",
    "            \n",
    "            #Verify there's something in it\n",
    "            if len(match) == 0:\n",
    "                print(f'\\t-No match ID for user {name}. Skipping this user!')\n",
    "                break\n",
    "            \n",
    "            #Convert match_id to integer\n",
    "            match_num = int(match[0].strip('NA1_'))\n",
    "            \n",
    "            #Compare current match_id to basegame var.\n",
    "            #Skip if match_id is less than basegame\n",
    "            if match_num < basegame:\n",
    "                print(f'\\t-Completed getting all current patch Match IDs for user {name}')\n",
    "                print(f'\\t-There are {len(name_list) - n} users left.')\n",
    "                break\n",
    "\n",
    "            ####################### Get Match Info ##########################\n",
    "            \n",
    "            #Now that we know the current match_id is part of the newest patch,\n",
    "            #We can retrieve the actual match info. Keep in mind this is still part\n",
    "            #of the while loop above\n",
    "                \n",
    "            #Every 250 entries, save the lists as json files and reset the lists\n",
    "            if len(timeline_data) % 250 == 0:    \n",
    "                #Save timeline data.\n",
    "                #First, convert it to a df\n",
    "                timeline_df = pd.DataFrame(timeline_data)\n",
    "\n",
    "                #Save as json file. Use username string to identify whose file it is\n",
    "                timeline_df.to_json(f'timeline_data_user_{name}_end_{num_json}.json')\n",
    "\n",
    "                #Print status message\n",
    "                print(f'\\t-Created timeline_data_user_{name}_end_{num_json}.json')\n",
    "\n",
    "                #Save other game data\n",
    "                #First, convert it to a df\n",
    "                game_df = pd.DataFrame(game_data)\n",
    "\n",
    "                #Save as json file. Use username string to identify whose file it is\n",
    "                game_df.to_json(f'other_game_data_user_{name}_end_{num_json}.json')\n",
    "\n",
    "                #Print status message\n",
    "                print(f'\\t-Created other_game_data_user_{name}_end_{num_json}.json\\n')\n",
    "                \n",
    "                #Now reset the temp lists\n",
    "                timeline_data = []\n",
    "                game_data = []\n",
    "            \n",
    "        \n",
    "            ############################# TIMELINE DATA #######################\n",
    "            #Set up timeline url\n",
    "            timeline_query = f'lol/match/v5/matches/{match[0]}/timeline/?api_key={api_key}'\n",
    "\n",
    "            #Grab timeline json data\n",
    "            timeline_response = requests.get(match_url + timeline_query)\n",
    "\n",
    "            #Update pull count and sleep for 150 seconds if necessary\n",
    "            num_pulls += 1\n",
    "\n",
    "            if num_pulls % 100 == 0:\n",
    "                print('\\nHit 100 pulls! Sleeping for 150 seconds.')\n",
    "                sleep(150)\n",
    "                print('Continuing!\\n')\n",
    "            \n",
    "            #Check response and leave a status message\n",
    "            if timeline_response.status_code != 200:\n",
    "                print(f'Something went wrong getting TIMELINE DATA! Status Code {timeline_response.status_code} for match ID: {match}.')\n",
    "                print(f'Skipping this match ID.\\n')\n",
    "                continue\n",
    "\n",
    "            #Turn it into json format\n",
    "            timeline_json = timeline_response.json()\n",
    "\n",
    "            #Append this data to the timeline_data list\n",
    "            timeline_data.append(timeline_json)\n",
    "            \n",
    "            ############################# OTHER GAME DATA #######################\n",
    "            #Set up game data url\n",
    "            game_query = f'lol/match/v5/matches/{match[0]}?api_key={api_key}'\n",
    "\n",
    "            #Grab game json data\n",
    "            game_response = requests.get(match_url + game_query)\n",
    "\n",
    "            #Update pull count and sleep for 150 seconds if necessary\n",
    "            num_pulls += 1\n",
    "\n",
    "            if num_pulls % 100 == 0:\n",
    "                print('\\nHit 100 pulls! Sleeping for 150 seconds.')\n",
    "                sleep(150)\n",
    "                print('Continuing!\\n')\n",
    "            \n",
    "            #Check response and leave a status message.\n",
    "            if game_response.status_code != 200:\n",
    "                print(f'Something went wrong getting OTHER GAME DATA! Status Code {game_response.status_code} for match ID: {match}.')\n",
    "                print(f'Skipping this match ID and REMOVING PREVIOUS TIMELINE ENTRY.\\n')\n",
    "\n",
    "                #Remove the last entry in the timeline_data list\n",
    "                timeline_data.pop()\n",
    "                continue\n",
    "\n",
    "            #Turn it into json format\n",
    "            game_json = game_response.json()\n",
    "\n",
    "            #Append this data to the game_data list\n",
    "            game_data.append(game_json)\n",
    "            \n",
    "            #Update num_json\n",
    "            num_json += 1\n",
    "            \n",
    "            #Update i\n",
    "            i += 1\n",
    "            \n",
    "        #Tell the user how many match_ids were used for the current user\n",
    "        print(f'\\t-Completed {i} Match IDs for user {name}\\n')\n",
    "    \n",
    "    #Save any json data that was not saved at the end of the for loop\n",
    "    \n",
    "    #Save timeline data.\n",
    "    #First, convert it to a df\n",
    "    timeline_df = pd.DataFrame(timeline_data)\n",
    "\n",
    "    #Save as json file. Use username string to identify whose file it is\n",
    "    timeline_df.to_json(f'timeline_data_user_{name}_end_{num_json}.json')\n",
    "\n",
    "    #Print status message\n",
    "    print(f'Created timeline_data_user_{name}_end_{num_json}.json')\n",
    "\n",
    "    #Save other game data\n",
    "    #First, convert it to a df\n",
    "    game_df = pd.DataFrame(game_data)\n",
    "\n",
    "    #Save as json file. Use username string to identify whose file it is\n",
    "    game_df.to_json(f'other_game_data_user_{name}_end_{num_json}.json')\n",
    "\n",
    "    #Print status message\n",
    "    print(f'Created other_game_data_user_{name}_end_{num_json}.json')\n",
    "    \n",
    "    #Complete! Leave a message\n",
    "    print('All users completed! Follow suit.')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398dacdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing, import the user name\n",
    "from pro_names import pro_names\n",
    "from env import api_key\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from env import api_key\n",
    "from time import sleep\n",
    "from acquire import extract, build_prepared_df, get_new_patch_data, get_players\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "pd.reset_option(\"display.max_rows\", \"display.max_columns\")\n",
    "import acquire\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89f8eb82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BJNA',\n",
       " 'BHSc',\n",
       " 'QAZx',\n",
       " 'Can Win N0 Need',\n",
       " 'lT0Tl',\n",
       " 'NA chopsticks',\n",
       " 'Hyacintho',\n",
       " 'Cawwo',\n",
       " 'LFT RCON',\n",
       " 'catnip1',\n",
       " 'Cyrus D',\n",
       " 'UF Trollzelda',\n",
       " 'Euphorian',\n",
       " 'Princess Mico',\n",
       " 'Jachsparrow',\n",
       " 'Hero of mischief',\n",
       " 'i will crush you',\n",
       " 'HAYES',\n",
       " 'GraceWangFinal18',\n",
       " 'BratEvolved',\n",
       " 'usu',\n",
       " 'SSAK',\n",
       " 'Adan',\n",
       " 'SC itssyaboinate',\n",
       " 'MeIody',\n",
       " 'LGM Snow',\n",
       " 'INSSA HUNTER',\n",
       " 'Young Tiller',\n",
       " 'nifty jg',\n",
       " 'FaceCheckOnBush',\n",
       " 'LPL x LCK Aggro',\n",
       " 'Z Z T A ',\n",
       " 'Youli',\n",
       " 'buhudong',\n",
       " 'thats fervor',\n",
       " 'PLEASEHELPADC',\n",
       " 'Augustine',\n",
       " 'ciri of cintra',\n",
       " 'Exiled Heretic',\n",
       " 'POTATOOOOOOOOO',\n",
       " 'Gweiss',\n",
       " 'Kyusung',\n",
       " 'TrevorKTran',\n",
       " 'Xiao Hun zii',\n",
       " 'Nanjingwaiguoyu',\n",
       " 'Weaker',\n",
       " 'MEMEME670',\n",
       " 'Fanthom',\n",
       " 'SuperKoolGus',\n",
       " 'eliron',\n",
       " 'SKT T1 Kaisaa',\n",
       " 'o2h',\n",
       " 'Nice Bunz',\n",
       " 'WQ only',\n",
       " 'Bio1',\n",
       " 'Swiftzzz',\n",
       " 'smallplaypi',\n",
       " 'Steam Sale',\n",
       " 'Elekktro',\n",
       " 'qwer123456',\n",
       " 'Two Beginnings',\n",
       " 'Xiao Suan Nai',\n",
       " 'Yellow dog',\n",
       " 'Venaz',\n",
       " 'Sam123',\n",
       " '1186',\n",
       " 'Spladaddy',\n",
       " 'Dahyun x Twice',\n",
       " 'AllGirlsRTheSame',\n",
       " 'jbraggs',\n",
       " 'Runepage',\n",
       " 'Breezyyy',\n",
       " 'Hypnotic Viper',\n",
       " 'TRUNDLE REBORN',\n",
       " 'Midwest Diamond',\n",
       " 'Aymmy',\n",
       " '72',\n",
       " '1982657489',\n",
       " 'KIDCUDI666',\n",
       " 'T21 Kedeck',\n",
       " '90CalBottomplug',\n",
       " 'Jerrel',\n",
       " 'Mamitas Puebla',\n",
       " 'Hi Im Famous',\n",
       " 'hyunta',\n",
       " 'Enthralled',\n",
       " 'Hung AYAYA',\n",
       " 'KingArtLeywin',\n",
       " 'ayuh1',\n",
       " 'Scenari0',\n",
       " 'shuoleza1jian',\n",
       " 'cematary',\n",
       " 'Alex Blais',\n",
       " 'Ineza',\n",
       " 'bu yao',\n",
       " 'small pp asian',\n",
       " 'OrangeManV2',\n",
       " 'Rocky9',\n",
       " 'ashmine',\n",
       " 'dear u',\n",
       " 'Unfamed JG',\n",
       " 'JulianC',\n",
       " 'Eshyn',\n",
       " 'UF trdblaster',\n",
       " 'Vesum',\n",
       " 'ttv 7Wylly7',\n",
       " 'OmBII',\n",
       " 'Daisoujou',\n",
       " 'AbrahamEst',\n",
       " 'Your Tea',\n",
       " 'Myx',\n",
       " 'Bubble Gum LCL',\n",
       " 'Immortal Kolg',\n",
       " 'Slamx',\n",
       " 'Lacrniez',\n",
       " 'JonnyRockets',\n",
       " 'BussyEnjoyerr808',\n",
       " 'BaconSeeker',\n",
       " 'colingogo',\n",
       " 'pprr',\n",
       " 'Kayn98',\n",
       " 'OGC PikaPika',\n",
       " 'picapica',\n",
       " 'Gaming Tanker',\n",
       " 'Alien Hominid',\n",
       " 'Doopliss1',\n",
       " 'saving na',\n",
       " 'AlphaFredo',\n",
       " 'KingOfAbeyance',\n",
       " 'Twitch BTangLoL',\n",
       " 'Oner v2',\n",
       " 'TTV LIVIDCA',\n",
       " 'chickeneater02',\n",
       " 'Valice',\n",
       " 'HydroZenith',\n",
       " 'sweety',\n",
       " 'Vortex03',\n",
       " 'HopeTilEnd',\n",
       " 'CroatianD',\n",
       " '17shou',\n",
       " 'Misterdot',\n",
       " 'hey im kevin',\n",
       " 'Camp this Bush',\n",
       " 'Dereams',\n",
       " 'My Ruru',\n",
       " 'hi its kay',\n",
       " 'Issys Cutie',\n",
       " 'Xytherior',\n",
       " 'Alesi',\n",
       " 'Gusenberg',\n",
       " 'Zach Riven',\n",
       " 'Hecti12',\n",
       " 'SteelCityWarrior',\n",
       " 'Alnair',\n",
       " 'CHlME',\n",
       " 'HaoyunXiongxiong',\n",
       " 'Witnesss Me',\n",
       " 'need happiness',\n",
       " 'Haru Urara',\n",
       " 'Eclipse',\n",
       " '33uler',\n",
       " 'Winaliel',\n",
       " 'Vortexual',\n",
       " 'Milky Way',\n",
       " 'Going Home',\n",
       " 'Akeha',\n",
       " 'Nebulus',\n",
       " 'markboots',\n",
       " 'Asherannea',\n",
       " 'Worlds',\n",
       " 'IvernedThisRank',\n",
       " 'zhubikiller',\n",
       " 'YellowNightmare',\n",
       " 'Zademort',\n",
       " 'Tempos Time',\n",
       " 'Devasis',\n",
       " 'emoyo',\n",
       " 'Chuty',\n",
       " 'jazzypie',\n",
       " 'AKA KW',\n",
       " 'DEMON FACT',\n",
       " 'stray kid',\n",
       " 'Geiko',\n",
       " 'Chookie',\n",
       " 'LolliGoesPop',\n",
       " 'cogii',\n",
       " 'Ronnie Dean',\n",
       " 'PUSHEEN DA LIMIT',\n",
       " 'Loyal',\n",
       " 'PickleTuna',\n",
       " 'A Horny Rhino',\n",
       " 'Yororong',\n",
       " 'Eunsoul',\n",
       " 'Garrett',\n",
       " 'C9 Klutch',\n",
       " 'Demon Back',\n",
       " 'Cameel',\n",
       " 'TimSevenhuysen',\n",
       " 'OuOOuO',\n",
       " '25z',\n",
       " 'Haha Lol',\n",
       " 'Amzer',\n",
       " 'Rio Kastle',\n",
       " 'hyperaggression',\n",
       " 'Piki',\n",
       " 'VT Cocoa',\n",
       " 'Mataz',\n",
       " 'Itsover man',\n",
       " 'cute as heck',\n",
       " 'MudBone69',\n",
       " 'hygge 1',\n",
       " 'Hakone',\n",
       " 'Silver3 High Elo',\n",
       " 'heck it mask off',\n",
       " 'bae girl',\n",
       " 'Inoriboob V1',\n",
       " 'Shaco Himself',\n",
       " 'saltmarwasmid',\n",
       " 'White Snow',\n",
       " 'VN Danny',\n",
       " 'Le chuL',\n",
       " 'WalIDashin',\n",
       " 'Lawton',\n",
       " 'Zylfer',\n",
       " 'MengMengBaBe',\n",
       " 'XiaoDanny',\n",
       " 'sasuke 2015',\n",
       " 'Gobta1',\n",
       " 'Wai Bi 8a Bo',\n",
       " 'Not Sengo',\n",
       " 'Ultrafilter',\n",
       " 'pings onIy',\n",
       " 'L9 6ix9ine69',\n",
       " 'Quidac',\n",
       " 'aniosaurus',\n",
       " 'AlphaMaleMelvin',\n",
       " 'br34th3',\n",
       " 'Requital x',\n",
       " 'NY Peng Yu Yan',\n",
       " 'Zech',\n",
       " 'y7b',\n",
       " 'rzd',\n",
       " 'pos2',\n",
       " 'perfunctory ToT',\n",
       " 'fClXmCeleste',\n",
       " 'typty',\n",
       " 'Matthew 19 24',\n",
       " 'DoYouLikeMilktea',\n",
       " 'MrEmerica',\n",
       " 'deathwalker222',\n",
       " 'TreeKangar00',\n",
       " 'AIlured By Day',\n",
       " 'oxhornqt',\n",
       " 'God of Supports',\n",
       " 'Espion',\n",
       " 'Zero Dark 30 23',\n",
       " 'Jido',\n",
       " 'Prestooob',\n",
       " 'Pupupu2',\n",
       " 'CantCarry',\n",
       " 'Lazzaro',\n",
       " 'MlKASA ACKERMANN',\n",
       " 'Love Elent',\n",
       " 'MoreMore',\n",
       " 'CaosFury',\n",
       " 'OztzyQ',\n",
       " 'urya',\n",
       " 'JustChilln',\n",
       " 'wanfeng',\n",
       " 'Dalseong',\n",
       " 'Kitzuo',\n",
       " 'March 14',\n",
       " 'Ke Ai XIao Mi',\n",
       " 'Zeptose',\n",
       " 'Lloth',\n",
       " 'sIaughter',\n",
       " 'Ponus',\n",
       " 'Xinthus',\n",
       " 'Auggy2627',\n",
       " 'Zha',\n",
       " 'Fake smiIes',\n",
       " 'B0BBY',\n",
       " 'Steefo',\n",
       " 'hotaze',\n",
       " 'CndyFlpnScintst',\n",
       " 'Dedasesne',\n",
       " 'onedyl',\n",
       " 'Millennium Baby',\n",
       " 'ChannyBoyy',\n",
       " 'DTC Jiansheng',\n",
       " 'OvOwOXD',\n",
       " 'future dreams',\n",
       " 'hotnas',\n",
       " 'Karasmai v9',\n",
       " 'Dance King Nunu',\n",
       " 'Duke9',\n",
       " 'wu wei wu ju',\n",
       " 'ALKSJHDLKCH',\n",
       " 'C9 Emilia',\n",
       " 'Never Story',\n",
       " 'Piggy COCO',\n",
       " 'VTG Meme Machine',\n",
       " 'Jons Jons',\n",
       " 'unhand thy lp',\n",
       " 'Vagery',\n",
       " 'Boji ddonggo cex',\n",
       " 'Coca PH',\n",
       " 'hmhsl',\n",
       " 'Janivia',\n",
       " 'small monster',\n",
       " 'Oops  A D  Gap',\n",
       " 'Hamster123',\n",
       " 'Hi im RBQ',\n",
       " 'Firelight',\n",
       " 'FreeXpHere',\n",
       " 'WP Conrad',\n",
       " 'Yluo',\n",
       " 'ColdCottageKoala',\n",
       " 'lIlIllIIllII',\n",
       " 'ttv Doglightning',\n",
       " 'santa obv',\n",
       " 'BorgvVz',\n",
       " 'Klln',\n",
       " 'Accrue',\n",
       " 'NeVerCaRry',\n",
       " 'Murillo Sama',\n",
       " 'SentFromAboveE',\n",
       " 'Scafe',\n",
       " 'one dream away',\n",
       " 'JDLBeast',\n",
       " 'TsundereGun',\n",
       " 'NEED GOTH GF PLZ',\n",
       " 'doompa',\n",
       " 'Thomason',\n",
       " 'Yao Bu Sheng Qi',\n",
       " 'Butterfan3',\n",
       " 'kaixinyidian',\n",
       " 'V1papa',\n",
       " 'EDG Jan Sali',\n",
       " 'Ronit',\n",
       " 'U Say Ur Horse',\n",
       " 'ObviouslyStupid',\n",
       " 'BecauseImAPotato',\n",
       " 'poor reasoning',\n",
       " 'Ken Carson',\n",
       " 'Prosciutto',\n",
       " 'wzy',\n",
       " 'FamousOneDay',\n",
       " 'Rainn',\n",
       " 'SNOwH1Te',\n",
       " '07 13 2003',\n",
       " 'XDME',\n",
       " 'Kyui',\n",
       " 'chenchenlol',\n",
       " 'Notzu',\n",
       " 'Xevris',\n",
       " 'Shockey',\n",
       " 'Sangydog',\n",
       " 'Dalun',\n",
       " 'Akamige',\n",
       " 'Dinosaurus',\n",
       " 'sprinklrr',\n",
       " 'YaaYeet',\n",
       " 'Gyshy',\n",
       " 'PerfectPerv0438',\n",
       " 'HopOffMyDonger',\n",
       " 'JattO',\n",
       " 'icelandl00000ve',\n",
       " 'Ha Yuri Zahard',\n",
       " 'poome1',\n",
       " 'hellstar remina',\n",
       " 'Rikudo',\n",
       " 'Ned',\n",
       " 'hong dou bing',\n",
       " 'Let me guide you',\n",
       " 'Boptimus',\n",
       " 'Chrixgg',\n",
       " 'Mimic v9',\n",
       " 'Cawv',\n",
       " 'PG Ace',\n",
       " 'PpineAapple',\n",
       " 'kbmistro675',\n",
       " 'Sketchdreams',\n",
       " 'KidNickarus',\n",
       " 'Get N Or Get Out',\n",
       " 'Dougie',\n",
       " 'A Joke To You',\n",
       " 'Mizt',\n",
       " 'ASCOOBYDOOSOBAD',\n",
       " 'sad goo',\n",
       " 'Swooshe',\n",
       " 'BlaoThorne RJ',\n",
       " 'For Lalisa',\n",
       " 'SKT T1 Oppa',\n",
       " 'Reigen',\n",
       " 'Jenny Mud Five',\n",
       " 'twitchtvELOSANTA',\n",
       " 'pradyuman',\n",
       " 'OGMemeLordV1',\n",
       " 'Excluwusive',\n",
       " 'The Master Yeezy',\n",
       " 'Meslo',\n",
       " 'Insane Lopez',\n",
       " 'No Safe Word',\n",
       " 'TTizzle830',\n",
       " 'Yami no Mi',\n",
       " 'USC Inferno ',\n",
       " 'The Lando',\n",
       " 'Abused Mid',\n",
       " 'LEAGUE OF ANDREW',\n",
       " 'itspumpkinbaby',\n",
       " 'Tom CPU',\n",
       " 'iarukas',\n",
       " 'hardstuck69',\n",
       " 'dog VallFire',\n",
       " 'Doing His Best',\n",
       " 'Meister Chief',\n",
       " 'Zoe pIs',\n",
       " 'Pooky Wan Kenobi',\n",
       " 'Citric',\n",
       " 'Crunchyfied',\n",
       " 'BladeCube',\n",
       " 'DontRageItsOk',\n",
       " 'MalevolentTitan',\n",
       " 'barnacleboy fan',\n",
       " 'gyu123987',\n",
       " 'Fiku',\n",
       " 'R0UFEN',\n",
       " 'Abusing Glacial',\n",
       " 'Amamikyu',\n",
       " 'Yurababy',\n",
       " 'chokey wakey',\n",
       " 'Betty Cheeks',\n",
       " 'Morl',\n",
       " 'love you weed',\n",
       " 'smoothjazzlover',\n",
       " 'Taste of You',\n",
       " 'Scrive',\n",
       " 'Phantom Hashira',\n",
       " 'FINDlNG TEEMO',\n",
       " 'Wonhae',\n",
       " 'Blokta',\n",
       " 'Saint SpoonKing',\n",
       " 'shinaru fan',\n",
       " 'ID Mana',\n",
       " 'Km1er',\n",
       " 'GuGubird',\n",
       " 'rinnegan mid',\n",
       " 'readysetgo1',\n",
       " 'Stiifo',\n",
       " 'SilverRitter',\n",
       " 'SOUPKITCHENCHEF',\n",
       " 'IlIlIlIlIIlIIlll',\n",
       " 'Sweetmiku',\n",
       " 'lIllllIIIl',\n",
       " 'DamnYouFine',\n",
       " 'Common Projects',\n",
       " 'Klexo',\n",
       " 'Fated Enigma',\n",
       " 'DumpieTruck',\n",
       " 'Li NaiNai  Sorry',\n",
       " '3uphoria',\n",
       " 'B E N J I L',\n",
       " 'Doaenel',\n",
       " 'Shawner',\n",
       " 'a china boy',\n",
       " 'SoIitified',\n",
       " 'FirstTimeCaitlyn',\n",
       " 'Aipotu',\n",
       " 'Jared Klein',\n",
       " 'CaptBear',\n",
       " 'SUPKING',\n",
       " 'rainjacket fish',\n",
       " 'Lonely Aganzo',\n",
       " 'Your Ruined King',\n",
       " 'Astocaza Deza',\n",
       " 'Fromunda123',\n",
       " 'keurig cup',\n",
       " 'ShyniRuo',\n",
       " 'Va1ue',\n",
       " 'AZQWER',\n",
       " 'HASHIRE SORI YO',\n",
       " 'Aspen',\n",
       " 'destiny9',\n",
       " 'disparage',\n",
       " 'Dangerously',\n",
       " 'Araidan',\n",
       " 'KJKook',\n",
       " 'Le Dawn',\n",
       " 'Ur LP Is Mine',\n",
       " 'ThePieCrusader',\n",
       " 'TimelessCub',\n",
       " 'SeattIe',\n",
       " 'Lefasa',\n",
       " 'The 25th Justin',\n",
       " '197',\n",
       " 'Waraight',\n",
       " 'WhatsInItForMe',\n",
       " 'Cannibaal Kevin',\n",
       " 'hi im cupcake',\n",
       " 'Niles',\n",
       " 'Fighter Torso',\n",
       " 'Whimsy434',\n",
       " 'K3v1nRul3s',\n",
       " 'I Love Band',\n",
       " 'Morelia',\n",
       " 'AlexKimmy',\n",
       " 'BoilTheOil',\n",
       " 'EDGHyper',\n",
       " 'donbray1',\n",
       " 'Protos',\n",
       " 'Kexi1',\n",
       " '700kg grandma',\n",
       " 'Kim Down',\n",
       " '5FlowerMeat',\n",
       " 'A5PECT',\n",
       " 'Ms Vanjie',\n",
       " 'Ram Best Girl',\n",
       " 'Invitrix',\n",
       " 'yyvy',\n",
       " 'SplashOfCrimson',\n",
       " 'Rang Wo Bai Lan',\n",
       " 'fdecnadtsal',\n",
       " 'C9 Darshan',\n",
       " 'WangYiBo',\n",
       " '9 inch cigar',\n",
       " 'punkstar',\n",
       " 'SisterJunky',\n",
       " 'Iuno',\n",
       " 'Spirax',\n",
       " 'Hakuho',\n",
       " 'Aechkay',\n",
       " 'Addle',\n",
       " 'perryjg',\n",
       " 'mohawk',\n",
       " 'S Monkey',\n",
       " 'Construct 00',\n",
       " 'season 11 crummy',\n",
       " 'PowerSmite',\n",
       " 'BurstPhantom42',\n",
       " 'in the gulag',\n",
       " 'Bashful Iceberg',\n",
       " 'Forty',\n",
       " 'JonBiven',\n",
       " 'Grovy',\n",
       " 'Cheerleader',\n",
       " 'TTVcourantee',\n",
       " 'iQ Aashkaash',\n",
       " 'hasaki warrior',\n",
       " 'Nara',\n",
       " 'mid ruin',\n",
       " 'Iuap',\n",
       " 'I Am Chanman',\n",
       " 'I EAT BANANAS',\n",
       " 'Yoshi',\n",
       " 'UCI duong pro',\n",
       " 'karasuno setter',\n",
       " 'CocoMisu',\n",
       " 'Max108',\n",
       " 'Suitofarm',\n",
       " 'Otis Burrell ',\n",
       " 'Tippey',\n",
       " 'ridley',\n",
       " '2 Spearz',\n",
       " 'Amate',\n",
       " 'Surround',\n",
       " 'youngfly ',\n",
       " 'ANaKedkid',\n",
       " 'TEMP 27236133',\n",
       " 'Flamboozle',\n",
       " '2k 1m80 20cm 30m',\n",
       " 'Exile',\n",
       " 'davidisape',\n",
       " 'rovex',\n",
       " 'Wan An Husky',\n",
       " 'huffalumps',\n",
       " 'Doctor Doom',\n",
       " 'bea4wgl',\n",
       " 'Dig Bick Dandito',\n",
       " 'JOKER IS BACK ',\n",
       " 'Nico is PeppaPig',\n",
       " 'ur mom r all fat',\n",
       " 'kidbizzle',\n",
       " 'Para Ti',\n",
       " 'lu28826167',\n",
       " 'LF Noona',\n",
       " 'big cat',\n",
       " 'ispy1',\n",
       " 'GotMyFuzzyCuffs',\n",
       " 'Celestic',\n",
       " 'hi im oreki',\n",
       " 'Yun 77',\n",
       " '2Doran1Ninjatabi',\n",
       " 'Disconnector',\n",
       " 'Wx Najuehuo',\n",
       " 'CrusaderX9',\n",
       " 'A Little Yue',\n",
       " 'Busio3',\n",
       " 'Kill Minion',\n",
       " 'BigLlama10',\n",
       " 'pissss',\n",
       " 'ADC Skillcap',\n",
       " 'Bad Healer',\n",
       " 'Stabsie',\n",
       " '0ops Iron player',\n",
       " 'TRAFFI UNLEASHED',\n",
       " 'CoIby',\n",
       " 'namjaya',\n",
       " 'sniffuM',\n",
       " 'TL Armao',\n",
       " 'Mizli',\n",
       " 'Nappy',\n",
       " 'Loves a game',\n",
       " 'swifty12',\n",
       " '25 hot',\n",
       " 'NoNoYogi',\n",
       " 'Mental Bloom',\n",
       " 'Lacunose',\n",
       " 'Everest76',\n",
       " 'InitialLotus',\n",
       " 'NanoSpeck v2',\n",
       " 'MrBrobo',\n",
       " 'ArKaDaTa',\n",
       " '100 Chad',\n",
       " 'Ekisde',\n",
       " 'Bluwunk',\n",
       " 'Nyankali',\n",
       " 'Yoonsa',\n",
       " 'Raveydemon',\n",
       " 'GSDFQWASD',\n",
       " 'Tutsz1',\n",
       " 'Secret PIayer',\n",
       " 'Yonder Sapphire',\n",
       " 'Darcreationx',\n",
       " 'needyhenry',\n",
       " 'ABCDEFGHiJKLMAO',\n",
       " 'MA Taelone',\n",
       " 'Thomas Shelby1',\n",
       " 'BirdofSand',\n",
       " 'PlayWith4Amputee',\n",
       " 'UltraFoxy',\n",
       " 'Pingky',\n",
       " 'Lynnk',\n",
       " 'Rosaceae',\n",
       " 'lce Truck',\n",
       " 'LockHP',\n",
       " 'BronzeVg',\n",
       " 'Pupu owo',\n",
       " 'Why So Seriously',\n",
       " 'Shapeshift',\n",
       " 'FongIuNie',\n",
       " 'Vasticious',\n",
       " 'Scarlet Symphony',\n",
       " 'old dry mother',\n",
       " 'im Ahj',\n",
       " 'Taffys',\n",
       " 'ShotcallerBilly1',\n",
       " 'strikerzone21',\n",
       " 'Viegod Sabroso',\n",
       " 'Cinnamon Bread',\n",
       " 'Qzy',\n",
       " 'Yasukeh',\n",
       " 'Mnew',\n",
       " 'I Like 02',\n",
       " 'GIchigo',\n",
       " 'Autmn',\n",
       " 'XYG  Xu Jian',\n",
       " 'A Real Kitty',\n",
       " 'Klixx',\n",
       " 'You Int I Sprint',\n",
       " 'Ai Love You Too',\n",
       " 'RiceLegend',\n",
       " 'mbti INFP',\n",
       " 'Asexual',\n",
       " 'xWind',\n",
       " '8SL Jiuji LieKai',\n",
       " 'Spurge',\n",
       " 'IlIllIlIllIII',\n",
       " 'Yun Qi Bao Bao',\n",
       " 'MyPetRicky',\n",
       " 'RoyalRoy',\n",
       " 'Pennacchio',\n",
       " 'Mongolia EQon',\n",
       " 'Fireflyy',\n",
       " 's11 peaker',\n",
       " 'Predator27',\n",
       " 'Iinks',\n",
       " 'ShadowAlkx',\n",
       " 'wickJKR',\n",
       " 'LunarPollo',\n",
       " 'always plan ahea',\n",
       " 'zdd',\n",
       " 'jhlam killer',\n",
       " 'Tringe',\n",
       " 'Gold Inferno',\n",
       " 'WXPOKEMONCATCHER',\n",
       " 'Kanao',\n",
       " 'play for lane',\n",
       " 'srtty9',\n",
       " 'Can1ne',\n",
       " 'All the way home',\n",
       " 'Dippie',\n",
       " 'Shweeb',\n",
       " '3681',\n",
       " 'SanguisAnguis',\n",
       " 'Falling',\n",
       " 'ahn jimin',\n",
       " 'Hot Kindred',\n",
       " 'kv5',\n",
       " 'AME JuiceTown',\n",
       " 'TheRealPhilip',\n",
       " 'jimball234',\n",
       " 'Junaaaa',\n",
       " 'Doowan',\n",
       " 'aHowtoPlayDota2',\n",
       " 'Cywdazbz',\n",
       " 'Xenobladeguy1337',\n",
       " 'Kyorez',\n",
       " 'Doglightning TV',\n",
       " 'liantiguaiwu',\n",
       " 'yeet yah',\n",
       " 'kisno',\n",
       " 'xHAYDEN',\n",
       " 'Tomgoku',\n",
       " 'omero combo 4',\n",
       " 'LeenK LCS',\n",
       " 'Radiance Hai',\n",
       " 'Azul Phoenix',\n",
       " 'lai gen hua z1',\n",
       " 'Tos8719',\n",
       " 'GONEBYSPRING',\n",
       " 'JJJD',\n",
       " 'ImNotVeryLegit',\n",
       " 'Aya Ucchi',\n",
       " 'Phrez',\n",
       " 'sena x',\n",
       " 'Autolykus',\n",
       " 'Zile',\n",
       " 'TheFatInTheCat',\n",
       " 'Im A Koala',\n",
       " 'Matty',\n",
       " 'Candice',\n",
       " 'LoveKatarinazzZ',\n",
       " 'Ralyk',\n",
       " 'shxchi fan 1',\n",
       " 'FLABREZU',\n",
       " 'Distoma',\n",
       " 'KKYY',\n",
       " 'Motelles',\n",
       " 'Guessmebaby',\n",
       " 'OUTOFYOURMIND',\n",
       " 'Dime',\n",
       " 'GoliathGames',\n",
       " 'poebettergame',\n",
       " 'NinjaAqua',\n",
       " 'archerboy123',\n",
       " 'swordyshield',\n",
       " 'ych1414',\n",
       " 'Ahri Inari',\n",
       " 'MarTea',\n",
       " 'wztzg',\n",
       " 'DemonicBug',\n",
       " 'blonde dahyun',\n",
       " 'ImpaIa',\n",
       " 'I Love Mandu',\n",
       " 'Zap Rowsdower',\n",
       " 'OvO ylc',\n",
       " 'Nyne',\n",
       " 'xHenzo',\n",
       " 'Dragoon',\n",
       " 'Le Kill',\n",
       " 'Frost Thunder',\n",
       " 'Diablow3251',\n",
       " 'SCOUTLOVESFROGS',\n",
       " 'GoldenNinja103',\n",
       " 'imk',\n",
       " 'Phrxshn',\n",
       " 'Reavered',\n",
       " 'sushibutt',\n",
       " 'Zoro 11',\n",
       " 'nsnwsw',\n",
       " 'Mathias with Jun',\n",
       " 'emiliacosplayfan',\n",
       " 'xsmako',\n",
       " 'Pick lulu thresh',\n",
       " 'CBootcy',\n",
       " 'corgi dorgi',\n",
       " 'AddictedToBacon',\n",
       " 'GeneralSn1per',\n",
       " 'no filter flamer',\n",
       " 'Kycaxisha',\n",
       " '200IQPLAYS',\n",
       " 'Went',\n",
       " 'Mobility',\n",
       " 'FJH',\n",
       " 'ShySept',\n",
       " 'ZombieOrange',\n",
       " 'Supersonics1',\n",
       " 'Shephurd',\n",
       " 'Bmfx',\n",
       " 'piwil',\n",
       " 'Rek It Ralph',\n",
       " 'Arneshie',\n",
       " 'PP Rear',\n",
       " 'Paracha',\n",
       " 'Manco',\n",
       " 'EDG Tonybaloney',\n",
       " 'QuinnAD',\n",
       " 'Annie117',\n",
       " 'Thats Not Fervor',\n",
       " 'Mario Sunshine',\n",
       " 'im new btw haha',\n",
       " 'Danyui',\n",
       " 'Assisted',\n",
       " 'TTV SaskioLoL',\n",
       " 'Tibbers69Party',\n",
       " 'HyyerCosmos',\n",
       " 'Arsenals',\n",
       " 'Ehowerew',\n",
       " 'TOUKAI TEIOU',\n",
       " 'Raydell',\n",
       " 'Twilight Ranger1',\n",
       " 'HeiHaiZi',\n",
       " 'Run Or lose',\n",
       " 'Byz',\n",
       " 'Phil Mechanics',\n",
       " 'parnelly',\n",
       " 'Barryyy',\n",
       " 'Meowing Cat',\n",
       " 'Melynlas',\n",
       " 'Kelpo',\n",
       " 'Ke Ie',\n",
       " 'Computer',\n",
       " 'Daybed',\n",
       " 'concept x',\n",
       " 'ItzAraGuys',\n",
       " 'wixaa',\n",
       " 'AniviaSmokesWeed',\n",
       " 'Fear',\n",
       " 'DarkWingsForSale',\n",
       " 'Trey Lance',\n",
       " 'Katfish',\n",
       " 'BlatantWintrader',\n",
       " 'hot girl irl',\n",
       " 'path top',\n",
       " 'OmegaK',\n",
       " 'MAMMOTHMAN65',\n",
       " 'No 2',\n",
       " 'IcFrosty9',\n",
       " 'parrks',\n",
       " 'itzz Wags 2',\n",
       " 'EgoDestroyer',\n",
       " 'SQKSQ Bottom',\n",
       " 'VIPERBIGANDA',\n",
       " 'B Lovee',\n",
       " 'Koggle',\n",
       " 'Yuumi Lover69UwU',\n",
       " 'GoshaBangBang',\n",
       " '2Maxim',\n",
       " 'me is who',\n",
       " 'zyw',\n",
       " 'TwTv Main Sup',\n",
       " 'vicky is cutest',\n",
       " 'BilboSwaggins6',\n",
       " 'AVAX BRIDGER',\n",
       " 'fakeryboy',\n",
       " 'Harbinger12',\n",
       " 'Lili0Valley',\n",
       " 'Waitmecarry',\n",
       " 'gwync',\n",
       " 'Gnome3G',\n",
       " 'Will 16',\n",
       " 'Mojo Jojo',\n",
       " '21 03 21',\n",
       " 'stacked q',\n",
       " 'ROYSH HACK XD',\n",
       " 'Semper',\n",
       " 'Sayarin',\n",
       " 'aquanthony',\n",
       " 'QNTA',\n",
       " 'CuJoMac',\n",
       " 'curtisisboss',\n",
       " 'Ran Out of Wit',\n",
       " 'clayd',\n",
       " 'Gumayusi wannaB',\n",
       " 'Kanshou',\n",
       " 'ASC Asylum',\n",
       " 'not playable',\n",
       " 'InsanityEnsues',\n",
       " 'lets rap',\n",
       " 'BoomBibi',\n",
       " '1764763',\n",
       " 'Hot Ruffles8',\n",
       " 'Scythorak',\n",
       " 'Angry game xD',\n",
       " 'Compskey',\n",
       " 'vetheo',\n",
       " '04 Jug',\n",
       " 'Best Turtle IND',\n",
       " 'saku fan',\n",
       " 'VEIG',\n",
       " 'S12 LCS',\n",
       " 'llllllllllllIIll',\n",
       " 'Marzion',\n",
       " 'Toidielpurp',\n",
       " 'WARCRIMINAL99',\n",
       " 'Rekeri',\n",
       " 'Lycho',\n",
       " 'Platypus3',\n",
       " 'Anime N Chill',\n",
       " 'UC Davis 2025',\n",
       " 'Darko',\n",
       " 'Shearrealtide',\n",
       " 'I0VE',\n",
       " 'KAJA1',\n",
       " 'Scruffy2400',\n",
       " 'liveevil',\n",
       " 'Fancy',\n",
       " 'NY Ma lao shi',\n",
       " 'oyuna',\n",
       " 'TTV StrawHatAni',\n",
       " 'Shortley',\n",
       " 'ArcticBird',\n",
       " 'need some Iuck',\n",
       " 'AgAthaa',\n",
       " 'IllIIIIlIIIlIIII',\n",
       " 'Enchanter',\n",
       " 'Garrett1',\n",
       " 'Twitter ThiefJG',\n",
       " 'Reckza',\n",
       " 'Power Of Friends',\n",
       " 'Number 1 Taco',\n",
       " 'Aevahom',\n",
       " 'Zud',\n",
       " 'glass heart',\n",
       " 'BengDi Bu BengDi',\n",
       " 'Sea Dragon',\n",
       " 'Throw',\n",
       " 'Hopefulx',\n",
       " 'emuhack',\n",
       " 'Volte',\n",
       " 'FA Bjergsen',\n",
       " '566',\n",
       " 'i am a girlboss',\n",
       " 'Efzo',\n",
       " 'cute mute',\n",
       " 'Crissp',\n",
       " 'XIAOZHUPEIHANG',\n",
       " 'mawlok2',\n",
       " 'i stomp on men',\n",
       " 'Raid Boss Morde',\n",
       " 'Loopsers',\n",
       " 'One Bad Monkey',\n",
       " 'Ninja Shen',\n",
       " '5tunt',\n",
       " 'WKFbaby',\n",
       " 'xAItF4',\n",
       " 'THAT IS ILLEGAL',\n",
       " 'neukda3',\n",
       " 'Rabbithasu',\n",
       " 'BOMBOM123',\n",
       " 'tibs2',\n",
       " 'Danyal',\n",
       " 'bogo binter',\n",
       " 'Love Xi',\n",
       " 'Apdo',\n",
       " 'Sosig',\n",
       " 'Sneewo',\n",
       " 'jknbov',\n",
       " 'SkiTzee',\n",
       " 'F33dMeKills',\n",
       " 'Bhola',\n",
       " 'May it be',\n",
       " 'Chalino Sanchez',\n",
       " 'Always Chase Me',\n",
       " 'THROW       LORD',\n",
       " 'Firetheft',\n",
       " 'Rhaast Asassin',\n",
       " 'Narichi',\n",
       " '1man4dog20ff',\n",
       " 'lron Pyrite',\n",
       " 'Yasaw',\n",
       " 'Anna Kragsloor',\n",
       " 'GhostJohnny',\n",
       " 'DiaKs',\n",
       " 'glizzysnacker',\n",
       " 'mid koldy',\n",
       " 'CR Rose',\n",
       " 'WafflesAndMilk',\n",
       " 'Almighty',\n",
       " 'SIGMA TOP',\n",
       " 'Bobbyy',\n",
       " 'huiyyao',\n",
       " 'HappySupportGuy',\n",
       " 'bie gao',\n",
       " 'Poopey',\n",
       " 'GDOG',\n",
       " 'Tasimony',\n",
       " 'Wero Teresa',\n",
       " 'zIceFire',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('5451_players.csv')\n",
    "player_list = df1.Player.to_list()\n",
    "player_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bea41b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = player_list[4174:4300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028fd377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Loop for user Bimz\n",
      "\t-Got the puuid for user Bimz.\n",
      "\t-Created timeline_data_user_Bimz_end_0.json\n",
      "\t-Created other_game_data_user_Bimz_end_0.json\n",
      "\n",
      "\t-Completed getting all current patch Match IDs for user Bimz\n",
      "\t-There are 125 users left.\n",
      "\t-Completed 3 Match IDs for user Bimz\n",
      "\n",
      "Beginning Loop for user Lil Hook\n",
      "\t-Got the puuid for user Lil Hook.\n",
      "\t-Completed getting all current patch Match IDs for user Lil Hook\n",
      "\t-There are 124 users left.\n",
      "\t-Completed 7 Match IDs for user Lil Hook\n",
      "\n",
      "Beginning Loop for user NuzziG\n",
      "\t-Got the puuid for user NuzziG.\n",
      "\t-Completed getting all current patch Match IDs for user NuzziG\n",
      "\t-There are 123 users left.\n",
      "\t-Completed 3 Match IDs for user NuzziG\n",
      "\n",
      "Beginning Loop for user emit arorua\n",
      "\t-Got the puuid for user emit arorua.\n",
      "\t-Completed getting all current patch Match IDs for user emit arorua\n",
      "\t-There are 122 users left.\n",
      "\t-Completed 7 Match IDs for user emit arorua\n",
      "\n",
      "Beginning Loop for user SYG Fei Fei Zi\n",
      "\t-Got the puuid for user SYG Fei Fei Zi.\n",
      "\t-Completed getting all current patch Match IDs for user SYG Fei Fei Zi\n",
      "\t-There are 121 users left.\n",
      "\t-Completed 0 Match IDs for user SYG Fei Fei Zi\n",
      "\n",
      "Beginning Loop for user Shienji\n",
      "\t-Got the puuid for user Shienji.\n",
      "\t-Completed getting all current patch Match IDs for user Shienji\n",
      "\t-There are 120 users left.\n",
      "\t-Completed 5 Match IDs for user Shienji\n",
      "\n",
      "Beginning Loop for user Keusios\n",
      "\t-Got the puuid for user Keusios.\n",
      "\n",
      "Hit 100 pulls! Sleeping for 150 seconds.\n",
      "Continuing!\n",
      "\n",
      "\t-Completed getting all current patch Match IDs for user Keusios\n",
      "\t-There are 119 users left.\n",
      "\t-Completed 10 Match IDs for user Keusios\n",
      "\n",
      "Beginning Loop for user TFBlade2\n",
      "\t-Got the puuid for user TFBlade2.\n",
      "\t-Completed getting all current patch Match IDs for user TFBlade2\n",
      "\t-There are 118 users left.\n",
      "\t-Completed 0 Match IDs for user TFBlade2\n",
      "\n",
      "Beginning Loop for user d1dayewanbuliao\n",
      "\t-Got the puuid for user d1dayewanbuliao.\n",
      "\t-Completed getting all current patch Match IDs for user d1dayewanbuliao\n",
      "\t-There are 117 users left.\n",
      "\t-Completed 11 Match IDs for user d1dayewanbuliao\n",
      "\n",
      "Beginning Loop for user DeadLee\n",
      "\t-Got the puuid for user DeadLee.\n",
      "\t-Completed getting all current patch Match IDs for user DeadLee\n",
      "\t-There are 116 users left.\n",
      "\t-Completed 5 Match IDs for user DeadLee\n",
      "\n",
      "Beginning Loop for user CrabAppleBoy\n",
      "\t-Got the puuid for user CrabAppleBoy.\n",
      "\n",
      "Hit 100 pulls! Sleeping for 150 seconds.\n"
     ]
    }
   ],
   "source": [
    "last_timeline_data, last_game_data = get_new_patch_data(test_list, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8a0a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the function\n",
    "def build_extracted_df(username, path = './', time = 10):\n",
    "    \"\"\"\n",
    "    This function will take in a path for the json files stored in your directory. \n",
    "    It will then read all of them into a list, convert them to lists of dicts, and feed\n",
    "    them into the extract function. The time variable is for the extract function and determines\n",
    "    at what timeframe the data will be acquired for. The username is only used for naming the\n",
    "    .csv file at the end. Please use your last name to avoid confusion.\n",
    "    \n",
    "    This function returns a dataframe with data extracted for the specified time. \n",
    "    It also automatically saves this dataframe as a .csv.\n",
    "    \n",
    "    The path variable defaults to the current directory.\n",
    "    The time variable defaults to the 15 minute mark.\n",
    "    \n",
    "    You will need to have os imported to run this function.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Gather the names of the timeline json files and sort them\n",
    "    #This will pull all files that start with 'timeline'\n",
    "    timeline_files = [ x for x in os.listdir(path) if x.startswith(\"timeline\") ]\n",
    "    timeline_files.sort()\n",
    "    \n",
    "    #Gather the names of the other game data json files and sort them\n",
    "    #This will pull all files that start with 'other'\n",
    "    other_data_files = [ x for x in os.listdir(path) if x.startswith(\"other\") ]\n",
    "    other_data_files.sort()\n",
    "    \n",
    "    #Will need an empty df to store the final df\n",
    "    extracted_df = pd.DataFrame()\n",
    "    \n",
    "    #Verify that the lists are the same length\n",
    "    if len(timeline_files) != len(other_data_files):\n",
    "        print('Something went wrong! The json lists are not equal!')\n",
    "        print('Stopping function.')\n",
    "        return\n",
    "\n",
    "    for i in range(0, len(timeline_files)):\n",
    "        #Need to read both of them into dataframes and then convert to list of dicts\n",
    "        timeline_list = pd.read_json(path + timeline_files[i])\n",
    "        game_list = pd.read_json(path + other_data_files[i])\n",
    "\n",
    "        #Convert to list of dicts\n",
    "        timeline_list = timeline_list.to_dict(orient = 'records')\n",
    "        game_list = game_list.to_dict(orient = 'records')\n",
    "\n",
    "        #Now feed them into Joshua C's prepare file. Extract data for the specified timeframe\n",
    "        temp_df = acquire.extract(timeline_list, game_list, time)\n",
    "\n",
    "        #Now append the temp_df to the complete_df\n",
    "        extracted_df = extracted_df.append(temp_df, ignore_index = True)\n",
    "            \n",
    "    #Drop duplicates and return the prepared df\n",
    "    extracted_df = extracted_df.drop_duplicates()\n",
    "    \n",
    "    #Now save the extracted_df as a .csv\n",
    "    extracted_df.to_csv(f'new_extracted_data_{username}.csv', index = False)\n",
    "    \n",
    "    return extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c09d6d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: 0 due to <10 min or not classic\n",
      "Skipping: 1 due to <10 min or not classic\n",
      "Skipping: 2 due to <10 min or not classic\n",
      "Skipping: 3 due to <10 min or not classic\n",
      "Skipping: 4 due to <10 min or not classic\n",
      "Skipping: 5 due to <10 min or not classic\n",
      "Finished with: 6 of 249\n",
      "Finished with: 7 of 249\n",
      "Skipping: 8 due to <10 min or not classic\n",
      "Finished with: 9 of 249\n",
      "Finished with: 10 of 249\n",
      "Skipping: 11 due to <10 min or not classic\n",
      "Finished with: 12 of 249\n",
      "Skipping: 13 due to <10 min or not classic\n",
      "Finished with: 14 of 249\n",
      "Skipping: 15 due to <10 min or not classic\n",
      "Finished with: 16 of 249\n",
      "Finished with: 17 of 249\n",
      "Skipping: 18 due to <10 min or not classic\n",
      "Finished with: 19 of 249\n",
      "Finished with: 20 of 249\n",
      "Finished with: 21 of 249\n",
      "Finished with: 22 of 249\n",
      "Skipping: 23 due to <10 min or not classic\n",
      "Skipping: 24 due to <10 min or not classic\n",
      "Skipping: 25 due to <10 min or not classic\n",
      "Skipping: 26 due to <10 min or not classic\n",
      "Skipping: 27 due to <10 min or not classic\n",
      "Skipping: 28 due to <10 min or not classic\n",
      "Skipping: 29 due to <10 min or not classic\n",
      "Skipping: 30 due to <10 min or not classic\n",
      "Skipping: 31 due to <10 min or not classic\n",
      "Skipping: 32 due to <10 min or not classic\n",
      "Finished with: 33 of 249\n",
      "Skipping: 34 due to <10 min or not classic\n",
      "Finished with: 35 of 249\n",
      "Finished with: 36 of 249\n",
      "Finished with: 37 of 249\n",
      "Finished with: 38 of 249\n",
      "Skipping: 39 due to <10 min or not classic\n",
      "Finished with: 40 of 249\n",
      "Finished with: 41 of 249\n",
      "Finished with: 42 of 249\n",
      "Finished with: 43 of 249\n",
      "Skipping: 44 due to <10 min or not classic\n",
      "Finished with: 45 of 249\n",
      "Finished with: 46 of 249\n",
      "Skipping: 47 due to <10 min or not classic\n",
      "Skipping: 48 due to <10 min or not classic\n",
      "Finished with: 49 of 249\n",
      "Finished with: 50 of 249\n",
      "Finished with: 51 of 249\n",
      "Finished with: 52 of 249\n",
      "Skipping: 53 due to <10 min or not classic\n",
      "Skipping: 54 due to <10 min or not classic\n",
      "Skipping: 55 due to <10 min or not classic\n",
      "Skipping: 56 due to <10 min or not classic\n",
      "Skipping: 57 due to <10 min or not classic\n",
      "Skipping: 58 due to <10 min or not classic\n",
      "Skipping: 59 due to <10 min or not classic\n",
      "Skipping: 60 due to <10 min or not classic\n",
      "Skipping: 61 due to <10 min or not classic\n",
      "Finished with: 62 of 249\n",
      "Skipping: 63 due to <10 min or not classic\n",
      "Skipping: 64 due to <10 min or not classic\n",
      "Finished with: 65 of 249\n",
      "Finished with: 66 of 249\n",
      "Skipping: 67 due to <10 min or not classic\n",
      "Finished with: 68 of 249\n",
      "Finished with: 69 of 249\n",
      "Finished with: 70 of 249\n",
      "Finished with: 71 of 249\n",
      "Finished with: 72 of 249\n",
      "Finished with: 73 of 249\n",
      "Finished with: 74 of 249\n",
      "Finished with: 75 of 249\n",
      "Finished with: 76 of 249\n",
      "Finished with: 77 of 249\n",
      "Finished with: 78 of 249\n",
      "Skipping: 79 due to <10 min or not classic\n",
      "Skipping: 80 due to <10 min or not classic\n",
      "Skipping: 81 due to <10 min or not classic\n",
      "Skipping: 82 due to <10 min or not classic\n",
      "Skipping: 83 due to <10 min or not classic\n",
      "Skipping: 84 due to <10 min or not classic\n",
      "Skipping: 85 due to <10 min or not classic\n",
      "Skipping: 86 due to <10 min or not classic\n",
      "Finished with: 87 of 249\n",
      "Skipping: 88 due to <10 min or not classic\n",
      "Skipping: 89 due to <10 min or not classic\n",
      "Skipping: 90 due to <10 min or not classic\n",
      "Finished with: 91 of 249\n",
      "Skipping: 92 due to <10 min or not classic\n",
      "Skipping: 93 due to <10 min or not classic\n",
      "Skipping: 94 due to <10 min or not classic\n",
      "Skipping: 95 due to <10 min or not classic\n",
      "Finished with: 96 of 249\n",
      "Skipping: 97 due to <10 min or not classic\n",
      "Skipping: 98 due to <10 min or not classic\n",
      "Finished with: 99 of 249\n",
      "Finished with: 100 of 249\n",
      "Finished with: 101 of 249\n",
      "Finished with: 102 of 249\n",
      "Skipping: 103 due to <10 min or not classic\n",
      "Finished with: 104 of 249\n",
      "Finished with: 105 of 249\n",
      "Finished with: 106 of 249\n",
      "Skipping: 107 due to <10 min or not classic\n",
      "Skipping: 108 due to <10 min or not classic\n",
      "Skipping: 109 due to <10 min or not classic\n",
      "Finished with: 110 of 249\n",
      "Skipping: 111 due to <10 min or not classic\n",
      "Skipping: 112 due to <10 min or not classic\n",
      "Skipping: 113 due to <10 min or not classic\n",
      "Skipping: 114 due to <10 min or not classic\n",
      "Finished with: 115 of 249\n",
      "Skipping: 116 due to <10 min or not classic\n",
      "Finished with: 117 of 249\n",
      "Finished with: 118 of 249\n",
      "Finished with: 119 of 249\n",
      "Skipping: 120 due to <10 min or not classic\n",
      "Skipping: 121 due to <10 min or not classic\n",
      "Finished with: 122 of 249\n",
      "Finished with: 123 of 249\n",
      "Finished with: 124 of 249\n",
      "Finished with: 125 of 249\n",
      "Skipping: 126 due to <10 min or not classic\n",
      "Skipping: 127 due to <10 min or not classic\n",
      "Skipping: 128 due to <10 min or not classic\n",
      "Finished with: 129 of 249\n",
      "Finished with: 130 of 249\n",
      "Skipping: 131 due to <10 min or not classic\n",
      "Finished with: 132 of 249\n",
      "Skipping: 133 due to <10 min or not classic\n",
      "Finished with: 134 of 249\n",
      "Finished with: 135 of 249\n",
      "Skipping: 136 due to <10 min or not classic\n",
      "Finished with: 137 of 249\n",
      "Finished with: 138 of 249\n",
      "Skipping: 139 due to <10 min or not classic\n",
      "Finished with: 140 of 249\n",
      "Finished with: 141 of 249\n",
      "Finished with: 142 of 249\n",
      "Finished with: 143 of 249\n",
      "Finished with: 144 of 249\n",
      "Skipping: 145 due to <10 min or not classic\n",
      "Skipping: 146 due to <10 min or not classic\n",
      "Skipping: 147 due to <10 min or not classic\n",
      "Finished with: 148 of 249\n",
      "Skipping: 149 due to <10 min or not classic\n",
      "Skipping: 150 due to <10 min or not classic\n",
      "Skipping: 151 due to <10 min or not classic\n",
      "Skipping: 152 due to <10 min or not classic\n",
      "Finished with: 153 of 249\n",
      "Finished with: 154 of 249\n",
      "Finished with: 155 of 249\n",
      "Skipping: 156 due to <10 min or not classic\n",
      "Finished with: 157 of 249\n",
      "Skipping: 158 due to <10 min or not classic\n",
      "Skipping: 159 due to <10 min or not classic\n",
      "Skipping: 160 due to <10 min or not classic\n",
      "Skipping: 161 due to <10 min or not classic\n",
      "Skipping: 162 due to <10 min or not classic\n",
      "Skipping: 163 due to <10 min or not classic\n",
      "Finished with: 164 of 249\n",
      "Finished with: 165 of 249\n",
      "Finished with: 166 of 249\n",
      "Finished with: 167 of 249\n",
      "Finished with: 168 of 249\n",
      "Finished with: 169 of 249\n",
      "Skipping: 170 due to <10 min or not classic\n",
      "Skipping: 171 due to <10 min or not classic\n",
      "Skipping: 172 due to <10 min or not classic\n",
      "Skipping: 173 due to <10 min or not classic\n",
      "Finished with: 174 of 249\n",
      "Finished with: 175 of 249\n",
      "Finished with: 176 of 249\n",
      "Skipping: 177 due to <10 min or not classic\n",
      "Skipping: 178 due to <10 min or not classic\n",
      "Skipping: 179 due to <10 min or not classic\n",
      "Skipping: 180 due to <10 min or not classic\n",
      "Skipping: 181 due to <10 min or not classic\n",
      "Skipping: 182 due to <10 min or not classic\n",
      "Skipping: 183 due to <10 min or not classic\n",
      "Skipping: 184 due to <10 min or not classic\n",
      "Skipping: 185 due to <10 min or not classic\n",
      "Skipping: 186 due to <10 min or not classic\n",
      "Skipping: 187 due to <10 min or not classic\n",
      "Skipping: 188 due to <10 min or not classic\n",
      "Skipping: 189 due to <10 min or not classic\n",
      "Skipping: 190 due to <10 min or not classic\n",
      "Skipping: 191 due to <10 min or not classic\n",
      "Skipping: 192 due to <10 min or not classic\n",
      "Skipping: 193 due to <10 min or not classic\n",
      "Skipping: 194 due to <10 min or not classic\n",
      "Skipping: 195 due to <10 min or not classic\n",
      "Skipping: 196 due to <10 min or not classic\n",
      "Skipping: 197 due to <10 min or not classic\n",
      "Skipping: 198 due to <10 min or not classic\n",
      "Skipping: 199 due to <10 min or not classic\n",
      "Skipping: 200 due to <10 min or not classic\n",
      "Skipping: 201 due to <10 min or not classic\n",
      "Skipping: 202 due to <10 min or not classic\n",
      "Skipping: 203 due to <10 min or not classic\n",
      "Skipping: 204 due to <10 min or not classic\n",
      "Skipping: 205 due to <10 min or not classic\n",
      "Skipping: 206 due to <10 min or not classic\n",
      "Finished with: 207 of 249\n",
      "Skipping: 208 due to <10 min or not classic\n",
      "Skipping: 209 due to <10 min or not classic\n",
      "Finished with: 210 of 249\n",
      "Skipping: 211 due to <10 min or not classic\n",
      "Finished with: 212 of 249\n",
      "Finished with: 213 of 249\n",
      "Finished with: 214 of 249\n",
      "Finished with: 215 of 249\n",
      "Skipping: 216 due to <10 min or not classic\n",
      "Skipping: 217 due to <10 min or not classic\n",
      "Finished with: 218 of 249\n",
      "Finished with: 219 of 249\n",
      "Skipping: 220 due to <10 min or not classic\n",
      "Finished with: 221 of 249\n",
      "Skipping: 222 due to <10 min or not classic\n",
      "Finished with: 223 of 249\n",
      "Finished with: 224 of 249\n",
      "Finished with: 225 of 249\n",
      "Skipping: 226 due to <10 min or not classic\n",
      "Skipping: 227 due to <10 min or not classic\n",
      "Skipping: 228 due to <10 min or not classic\n",
      "Skipping: 229 due to <10 min or not classic\n",
      "Skipping: 230 due to <10 min or not classic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with: 231 of 249\n",
      "Finished with: 232 of 249\n",
      "Finished with: 233 of 249\n",
      "Finished with: 234 of 249\n",
      "Finished with: 235 of 249\n",
      "Skipping: 236 due to <10 min or not classic\n",
      "Skipping: 237 due to <10 min or not classic\n",
      "Skipping: 238 due to <10 min or not classic\n",
      "Skipping: 239 due to <10 min or not classic\n",
      "Skipping: 240 due to <10 min or not classic\n",
      "Skipping: 241 due to <10 min or not classic\n",
      "Finished with: 242 of 249\n",
      "Skipping: 243 due to <10 min or not classic\n",
      "Skipping: 244 due to <10 min or not classic\n",
      "Skipping: 245 due to <10 min or not classic\n",
      "Skipping: 246 due to <10 min or not classic\n",
      "Finished with: 247 of 249\n",
      "Finished with: 248 of 249\n",
      "Finished with: 249 of 249\n",
      "Done! now lets get some korean bbq, more chicken plz! haha....follow suit\n",
      "Finished with: 0 of 249\n",
      "Finished with: 1 of 249\n",
      "Finished with: 2 of 249\n",
      "Skipping: 3 due to <10 min or not classic\n",
      "Skipping: 4 due to <10 min or not classic\n",
      "Skipping: 5 due to <10 min or not classic\n",
      "Finished with: 6 of 249\n",
      "Skipping: 7 due to <10 min or not classic\n",
      "Finished with: 8 of 249\n",
      "Finished with: 9 of 249\n",
      "Finished with: 10 of 249\n",
      "Finished with: 11 of 249\n",
      "Finished with: 12 of 249\n",
      "Skipping: 13 due to <10 min or not classic\n",
      "Finished with: 14 of 249\n",
      "Skipping: 15 due to <10 min or not classic\n",
      "Finished with: 16 of 249\n",
      "Finished with: 17 of 249\n",
      "Finished with: 18 of 249\n",
      "Skipping: 19 due to <10 min or not classic\n",
      "Finished with: 20 of 249\n",
      "Finished with: 21 of 249\n",
      "Finished with: 22 of 249\n",
      "Finished with: 23 of 249\n",
      "Skipping: 24 due to <10 min or not classic\n",
      "Skipping: 25 due to <10 min or not classic\n",
      "Finished with: 26 of 249\n",
      "Finished with: 27 of 249\n",
      "Finished with: 28 of 249\n",
      "Finished with: 29 of 249\n",
      "Skipping: 30 due to <10 min or not classic\n",
      "Skipping: 31 due to <10 min or not classic\n",
      "Skipping: 32 due to <10 min or not classic\n",
      "Skipping: 33 due to <10 min or not classic\n",
      "Skipping: 34 due to <10 min or not classic\n",
      "Skipping: 35 due to <10 min or not classic\n",
      "Skipping: 36 due to <10 min or not classic\n",
      "Skipping: 37 due to <10 min or not classic\n",
      "Skipping: 38 due to <10 min or not classic\n",
      "Skipping: 39 due to <10 min or not classic\n",
      "Finished with: 40 of 249\n",
      "Finished with: 41 of 249\n",
      "Skipping: 42 due to <10 min or not classic\n",
      "Finished with: 43 of 249\n",
      "Finished with: 44 of 249\n",
      "Finished with: 45 of 249\n",
      "Skipping: 46 due to <10 min or not classic\n",
      "Skipping: 47 due to <10 min or not classic\n",
      "Skipping: 48 due to <10 min or not classic\n",
      "Skipping: 49 due to <10 min or not classic\n",
      "Finished with: 50 of 249\n",
      "Finished with: 51 of 249\n",
      "Skipping: 52 due to <10 min or not classic\n",
      "Skipping: 53 due to <10 min or not classic\n",
      "Skipping: 54 due to <10 min or not classic\n",
      "Skipping: 55 due to <10 min or not classic\n",
      "Skipping: 56 due to <10 min or not classic\n",
      "Finished with: 57 of 249\n",
      "Skipping: 58 due to <10 min or not classic\n",
      "Skipping: 59 due to <10 min or not classic\n",
      "Skipping: 60 due to <10 min or not classic\n",
      "Finished with: 61 of 249\n",
      "Skipping: 62 due to <10 min or not classic\n",
      "Finished with: 63 of 249\n",
      "Finished with: 64 of 249\n",
      "Finished with: 65 of 249\n",
      "Finished with: 66 of 249\n",
      "Finished with: 67 of 249\n",
      "Skipping: 68 due to <10 min or not classic\n",
      "Skipping: 69 due to <10 min or not classic\n",
      "Skipping: 70 due to <10 min or not classic\n",
      "Finished with: 71 of 249\n",
      "Finished with: 72 of 249\n",
      "Skipping: 73 due to <10 min or not classic\n",
      "Finished with: 74 of 249\n",
      "Finished with: 75 of 249\n",
      "Finished with: 76 of 249\n",
      "Finished with: 77 of 249\n",
      "Skipping: 78 due to <10 min or not classic\n",
      "Finished with: 79 of 249\n",
      "Finished with: 80 of 249\n",
      "Skipping: 81 due to <10 min or not classic\n",
      "Skipping: 82 due to <10 min or not classic\n",
      "Finished with: 83 of 249\n",
      "Skipping: 84 due to <10 min or not classic\n",
      "Skipping: 85 due to <10 min or not classic\n",
      "Finished with: 86 of 249\n",
      "Skipping: 87 due to <10 min or not classic\n",
      "Finished with: 88 of 249\n",
      "Finished with: 89 of 249\n",
      "Finished with: 90 of 249\n",
      "Finished with: 91 of 249\n",
      "Finished with: 92 of 249\n",
      "Finished with: 93 of 249\n",
      "Skipping: 94 due to <10 min or not classic\n",
      "Skipping: 95 due to <10 min or not classic\n",
      "Finished with: 96 of 249\n",
      "Finished with: 97 of 249\n",
      "Finished with: 98 of 249\n",
      "Skipping: 99 due to <10 min or not classic\n",
      "Skipping: 100 due to <10 min or not classic\n",
      "Finished with: 101 of 249\n",
      "Skipping: 102 due to <10 min or not classic\n",
      "Finished with: 103 of 249\n",
      "Finished with: 104 of 249\n",
      "Finished with: 105 of 249\n",
      "Finished with: 106 of 249\n",
      "Skipping: 107 due to <10 min or not classic\n",
      "Skipping: 108 due to <10 min or not classic\n",
      "Finished with: 109 of 249\n",
      "Finished with: 110 of 249\n",
      "Finished with: 111 of 249\n",
      "Finished with: 112 of 249\n",
      "Skipping: 113 due to <10 min or not classic\n",
      "Skipping: 114 due to <10 min or not classic\n",
      "Skipping: 115 due to <10 min or not classic\n",
      "Skipping: 116 due to <10 min or not classic\n",
      "Skipping: 117 due to <10 min or not classic\n",
      "Finished with: 118 of 249\n",
      "Finished with: 119 of 249\n",
      "Finished with: 120 of 249\n",
      "Finished with: 121 of 249\n",
      "Skipping: 122 due to <10 min or not classic\n",
      "Finished with: 123 of 249\n",
      "Finished with: 124 of 249\n",
      "Skipping: 125 due to <10 min or not classic\n",
      "Skipping: 126 due to <10 min or not classic\n",
      "Finished with: 127 of 249\n",
      "Finished with: 128 of 249\n",
      "Skipping: 129 due to <10 min or not classic\n",
      "Skipping: 130 due to <10 min or not classic\n",
      "Skipping: 131 due to <10 min or not classic\n",
      "Skipping: 132 due to <10 min or not classic\n",
      "Skipping: 133 due to <10 min or not classic\n",
      "Skipping: 134 due to <10 min or not classic\n",
      "Skipping: 135 due to <10 min or not classic\n",
      "Skipping: 136 due to <10 min or not classic\n",
      "Finished with: 137 of 249\n",
      "Finished with: 138 of 249\n",
      "Finished with: 139 of 249\n",
      "Skipping: 140 due to <10 min or not classic\n",
      "Skipping: 141 due to <10 min or not classic\n",
      "Skipping: 142 due to <10 min or not classic\n",
      "Skipping: 143 due to <10 min or not classic\n",
      "Skipping: 144 due to <10 min or not classic\n",
      "Skipping: 145 due to <10 min or not classic\n",
      "Finished with: 146 of 249\n",
      "Finished with: 147 of 249\n",
      "Finished with: 148 of 249\n",
      "Skipping: 149 due to <10 min or not classic\n",
      "Skipping: 150 due to <10 min or not classic\n",
      "Skipping: 151 due to <10 min or not classic\n",
      "Skipping: 152 due to <10 min or not classic\n",
      "Skipping: 153 due to <10 min or not classic\n",
      "Skipping: 154 due to <10 min or not classic\n",
      "Skipping: 155 due to <10 min or not classic\n",
      "Skipping: 156 due to <10 min or not classic\n",
      "Skipping: 157 due to <10 min or not classic\n",
      "Finished with: 158 of 249\n",
      "Skipping: 159 due to <10 min or not classic\n",
      "Skipping: 160 due to <10 min or not classic\n",
      "Finished with: 161 of 249\n",
      "Finished with: 162 of 249\n",
      "Finished with: 163 of 249\n",
      "Skipping: 164 due to <10 min or not classic\n",
      "Skipping: 165 due to <10 min or not classic\n",
      "Skipping: 166 due to <10 min or not classic\n",
      "Finished with: 167 of 249\n",
      "Finished with: 168 of 249\n",
      "Finished with: 169 of 249\n",
      "Skipping: 170 due to <10 min or not classic\n",
      "Finished with: 171 of 249\n",
      "Finished with: 172 of 249\n",
      "Finished with: 173 of 249\n",
      "Finished with: 174 of 249\n",
      "Skipping: 175 due to <10 min or not classic\n",
      "Skipping: 176 due to <10 min or not classic\n",
      "Skipping: 177 due to <10 min or not classic\n",
      "Skipping: 178 due to <10 min or not classic\n",
      "Skipping: 179 due to <10 min or not classic\n",
      "Skipping: 180 due to <10 min or not classic\n",
      "Skipping: 181 due to <10 min or not classic\n",
      "Skipping: 182 due to <10 min or not classic\n",
      "Skipping: 183 due to <10 min or not classic\n",
      "Skipping: 184 due to <10 min or not classic\n",
      "Skipping: 185 due to <10 min or not classic\n",
      "Skipping: 186 due to <10 min or not classic\n",
      "Skipping: 187 due to <10 min or not classic\n",
      "Skipping: 188 due to <10 min or not classic\n",
      "Skipping: 189 due to <10 min or not classic\n",
      "Skipping: 190 due to <10 min or not classic\n",
      "Skipping: 191 due to <10 min or not classic\n",
      "Skipping: 192 due to <10 min or not classic\n",
      "Skipping: 193 due to <10 min or not classic\n",
      "Skipping: 194 due to <10 min or not classic\n",
      "Skipping: 195 due to <10 min or not classic\n",
      "Skipping: 196 due to <10 min or not classic\n",
      "Skipping: 197 due to <10 min or not classic\n",
      "Skipping: 198 due to <10 min or not classic\n",
      "Skipping: 199 due to <10 min or not classic\n",
      "Skipping: 200 due to <10 min or not classic\n",
      "Skipping: 201 due to <10 min or not classic\n",
      "Skipping: 202 due to <10 min or not classic\n",
      "Skipping: 203 due to <10 min or not classic\n",
      "Skipping: 204 due to <10 min or not classic\n",
      "Skipping: 205 due to <10 min or not classic\n",
      "Skipping: 206 due to <10 min or not classic\n",
      "Skipping: 207 due to <10 min or not classic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with: 208 of 249\n",
      "Finished with: 209 of 249\n",
      "Skipping: 210 due to <10 min or not classic\n",
      "Skipping: 211 due to <10 min or not classic\n",
      "Skipping: 212 due to <10 min or not classic\n",
      "Skipping: 213 due to <10 min or not classic\n",
      "Finished with: 214 of 249\n",
      "Finished with: 215 of 249\n",
      "Skipping: 216 due to <10 min or not classic\n",
      "Finished with: 217 of 249\n",
      "Finished with: 218 of 249\n",
      "Skipping: 219 due to <10 min or not classic\n",
      "Skipping: 220 due to <10 min or not classic\n",
      "Skipping: 221 due to <10 min or not classic\n",
      "Skipping: 222 due to <10 min or not classic\n",
      "Finished with: 223 of 249\n",
      "Skipping: 224 due to <10 min or not classic\n",
      "Finished with: 225 of 249\n",
      "Finished with: 226 of 249\n",
      "Finished with: 227 of 249\n",
      "Finished with: 228 of 249\n",
      "Finished with: 229 of 249\n",
      "Skipping: 230 due to <10 min or not classic\n",
      "Skipping: 231 due to <10 min or not classic\n",
      "Skipping: 232 due to <10 min or not classic\n",
      "Skipping: 233 due to <10 min or not classic\n",
      "Skipping: 234 due to <10 min or not classic\n",
      "Skipping: 235 due to <10 min or not classic\n",
      "Skipping: 236 due to <10 min or not classic\n",
      "Skipping: 237 due to <10 min or not classic\n",
      "Skipping: 238 due to <10 min or not classic\n",
      "Skipping: 239 due to <10 min or not classic\n",
      "Skipping: 240 due to <10 min or not classic\n",
      "Skipping: 241 due to <10 min or not classic\n",
      "Skipping: 242 due to <10 min or not classic\n",
      "Skipping: 243 due to <10 min or not classic\n",
      "Skipping: 244 due to <10 min or not classic\n",
      "Skipping: 245 due to <10 min or not classic\n",
      "Skipping: 246 due to <10 min or not classic\n",
      "Skipping: 247 due to <10 min or not classic\n",
      "Skipping: 248 due to <10 min or not classic\n",
      "Skipping: 249 due to <10 min or not classic\n",
      "Done! now lets get some korean bbq, more chicken plz! haha....follow suit\n",
      "Done! now lets get some korean bbq, more chicken plz! haha....follow suit\n",
      "Skipping: 0 due to <10 min or not classic\n",
      "Skipping: 1 due to <10 min or not classic\n",
      "Skipping: 2 due to <10 min or not classic\n",
      "Skipping: 3 due to <10 min or not classic\n",
      "Skipping: 4 due to <10 min or not classic\n",
      "Skipping: 5 due to <10 min or not classic\n",
      "Skipping: 6 due to <10 min or not classic\n",
      "Skipping: 7 due to <10 min or not classic\n",
      "Finished with: 8 of 249\n",
      "Finished with: 9 of 249\n",
      "Finished with: 10 of 249\n",
      "Finished with: 11 of 249\n",
      "Skipping: 12 due to <10 min or not classic\n",
      "Finished with: 13 of 249\n",
      "Finished with: 14 of 249\n",
      "Finished with: 15 of 249\n",
      "Finished with: 16 of 249\n",
      "Skipping: 17 due to <10 min or not classic\n",
      "Skipping: 18 due to <10 min or not classic\n",
      "Finished with: 19 of 249\n",
      "Skipping: 20 due to <10 min or not classic\n",
      "Skipping: 21 due to <10 min or not classic\n",
      "Skipping: 22 due to <10 min or not classic\n",
      "Skipping: 23 due to <10 min or not classic\n",
      "Skipping: 24 due to <10 min or not classic\n",
      "Skipping: 25 due to <10 min or not classic\n",
      "Finished with: 26 of 249\n",
      "Finished with: 27 of 249\n",
      "Skipping: 28 due to <10 min or not classic\n",
      "Skipping: 29 due to <10 min or not classic\n",
      "Skipping: 30 due to <10 min or not classic\n",
      "Skipping: 31 due to <10 min or not classic\n",
      "Finished with: 32 of 249\n",
      "Skipping: 33 due to <10 min or not classic\n",
      "Skipping: 34 due to <10 min or not classic\n",
      "Finished with: 35 of 249\n",
      "Finished with: 36 of 249\n",
      "Finished with: 37 of 249\n",
      "Finished with: 38 of 249\n",
      "Skipping: 39 due to <10 min or not classic\n",
      "Finished with: 40 of 249\n",
      "Skipping: 41 due to <10 min or not classic\n",
      "Finished with: 42 of 249\n",
      "Finished with: 43 of 249\n",
      "Skipping: 44 due to <10 min or not classic\n",
      "Finished with: 45 of 249\n",
      "Skipping: 46 due to <10 min or not classic\n",
      "Finished with: 47 of 249\n",
      "Skipping: 48 due to <10 min or not classic\n",
      "Skipping: 49 due to <10 min or not classic\n",
      "Skipping: 50 due to <10 min or not classic\n",
      "Skipping: 51 due to <10 min or not classic\n",
      "Finished with: 52 of 249\n",
      "Skipping: 53 due to <10 min or not classic\n",
      "Skipping: 54 due to <10 min or not classic\n",
      "Skipping: 55 due to <10 min or not classic\n",
      "Skipping: 56 due to <10 min or not classic\n",
      "Skipping: 57 due to <10 min or not classic\n",
      "Skipping: 58 due to <10 min or not classic\n",
      "Skipping: 59 due to <10 min or not classic\n",
      "Finished with: 60 of 249\n",
      "Skipping: 61 due to <10 min or not classic\n",
      "Finished with: 62 of 249\n",
      "Skipping: 63 due to <10 min or not classic\n",
      "Finished with: 64 of 249\n",
      "Finished with: 65 of 249\n",
      "Finished with: 66 of 249\n",
      "Finished with: 67 of 249\n",
      "Skipping: 68 due to <10 min or not classic\n",
      "Finished with: 69 of 249\n",
      "Finished with: 70 of 249\n",
      "Skipping: 71 due to <10 min or not classic\n",
      "Skipping: 72 due to <10 min or not classic\n",
      "Skipping: 73 due to <10 min or not classic\n",
      "Skipping: 74 due to <10 min or not classic\n",
      "Skipping: 75 due to <10 min or not classic\n",
      "Skipping: 76 due to <10 min or not classic\n",
      "Skipping: 77 due to <10 min or not classic\n",
      "Finished with: 78 of 249\n",
      "Finished with: 79 of 249\n",
      "Finished with: 80 of 249\n",
      "Finished with: 81 of 249\n",
      "Finished with: 82 of 249\n",
      "Finished with: 83 of 249\n",
      "Finished with: 84 of 249\n",
      "Skipping: 85 due to <10 min or not classic\n",
      "Skipping: 86 due to <10 min or not classic\n",
      "Finished with: 87 of 249\n",
      "Skipping: 88 due to <10 min or not classic\n",
      "Finished with: 89 of 249\n",
      "Finished with: 90 of 249\n",
      "Finished with: 91 of 249\n",
      "Finished with: 92 of 249\n",
      "Skipping: 93 due to <10 min or not classic\n",
      "Skipping: 94 due to <10 min or not classic\n",
      "Skipping: 95 due to <10 min or not classic\n",
      "Finished with: 96 of 249\n",
      "Finished with: 97 of 249\n",
      "Finished with: 98 of 249\n",
      "Finished with: 99 of 249\n",
      "Finished with: 100 of 249\n",
      "Finished with: 101 of 249\n",
      "Finished with: 102 of 249\n",
      "Skipping: 103 due to <10 min or not classic\n",
      "Finished with: 104 of 249\n",
      "Finished with: 105 of 249\n",
      "Skipping: 106 due to <10 min or not classic\n",
      "Finished with: 107 of 249\n",
      "Finished with: 108 of 249\n",
      "Finished with: 109 of 249\n",
      "Finished with: 110 of 249\n",
      "Finished with: 111 of 249\n",
      "Finished with: 112 of 249\n",
      "Finished with: 113 of 249\n",
      "Skipping: 114 due to <10 min or not classic\n",
      "Skipping: 115 due to <10 min or not classic\n",
      "Skipping: 116 due to <10 min or not classic\n",
      "Skipping: 117 due to <10 min or not classic\n",
      "Finished with: 118 of 249\n",
      "Skipping: 119 due to <10 min or not classic\n",
      "Finished with: 120 of 249\n",
      "Finished with: 121 of 249\n",
      "Skipping: 122 due to <10 min or not classic\n",
      "Skipping: 123 due to <10 min or not classic\n",
      "Skipping: 124 due to <10 min or not classic\n",
      "Skipping: 125 due to <10 min or not classic\n",
      "Skipping: 126 due to <10 min or not classic\n",
      "Skipping: 127 due to <10 min or not classic\n",
      "Skipping: 128 due to <10 min or not classic\n",
      "Finished with: 129 of 249\n",
      "Finished with: 130 of 249\n",
      "Finished with: 131 of 249\n",
      "Finished with: 132 of 249\n",
      "Skipping: 133 due to <10 min or not classic\n",
      "Finished with: 134 of 249\n",
      "Finished with: 135 of 249\n",
      "Skipping: 136 due to <10 min or not classic\n",
      "Skipping: 137 due to <10 min or not classic\n",
      "Skipping: 138 due to <10 min or not classic\n",
      "Finished with: 139 of 249\n",
      "Finished with: 140 of 249\n",
      "Finished with: 141 of 249\n",
      "Finished with: 142 of 249\n",
      "Finished with: 143 of 249\n",
      "Skipping: 144 due to <10 min or not classic\n",
      "Finished with: 145 of 249\n",
      "Skipping: 146 due to <10 min or not classic\n",
      "Finished with: 147 of 249\n",
      "Skipping: 148 due to <10 min or not classic\n",
      "Skipping: 149 due to <10 min or not classic\n",
      "Finished with: 150 of 249\n",
      "Finished with: 151 of 249\n",
      "Finished with: 152 of 249\n",
      "Skipping: 153 due to <10 min or not classic\n",
      "Skipping: 154 due to <10 min or not classic\n",
      "Skipping: 155 due to <10 min or not classic\n",
      "Skipping: 156 due to <10 min or not classic\n",
      "Skipping: 157 due to <10 min or not classic\n",
      "Finished with: 158 of 249\n",
      "Finished with: 159 of 249\n",
      "Finished with: 160 of 249\n",
      "Finished with: 161 of 249\n",
      "Finished with: 162 of 249\n",
      "Skipping: 163 due to <10 min or not classic\n",
      "Finished with: 164 of 249\n",
      "Finished with: 165 of 249\n",
      "Finished with: 166 of 249\n",
      "Finished with: 167 of 249\n",
      "Finished with: 168 of 249\n",
      "Finished with: 169 of 249\n",
      "Finished with: 170 of 249\n",
      "Finished with: 171 of 249\n",
      "Skipping: 172 due to <10 min or not classic\n",
      "Skipping: 173 due to <10 min or not classic\n",
      "Finished with: 174 of 249\n",
      "Finished with: 175 of 249\n",
      "Finished with: 176 of 249\n",
      "Finished with: 177 of 249\n",
      "Skipping: 178 due to <10 min or not classic\n",
      "Skipping: 179 due to <10 min or not classic\n",
      "Finished with: 180 of 249\n",
      "Skipping: 181 due to <10 min or not classic\n",
      "Skipping: 182 due to <10 min or not classic\n",
      "Skipping: 183 due to <10 min or not classic\n",
      "Finished with: 184 of 249\n",
      "Skipping: 185 due to <10 min or not classic\n",
      "Skipping: 186 due to <10 min or not classic\n",
      "Skipping: 187 due to <10 min or not classic\n",
      "Skipping: 188 due to <10 min or not classic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished with: 189 of 249\n",
      "Finished with: 190 of 249\n",
      "Finished with: 191 of 249\n",
      "Finished with: 192 of 249\n",
      "Finished with: 193 of 249\n",
      "Skipping: 194 due to <10 min or not classic\n",
      "Finished with: 195 of 249\n",
      "Finished with: 196 of 249\n",
      "Finished with: 197 of 249\n",
      "Finished with: 198 of 249\n",
      "Finished with: 199 of 249\n",
      "Finished with: 200 of 249\n",
      "Finished with: 201 of 249\n",
      "Finished with: 202 of 249\n",
      "Skipping: 203 due to <10 min or not classic\n",
      "Finished with: 204 of 249\n",
      "Finished with: 205 of 249\n",
      "Finished with: 206 of 249\n",
      "Skipping: 207 due to <10 min or not classic\n",
      "Finished with: 208 of 249\n",
      "Finished with: 209 of 249\n",
      "Skipping: 210 due to <10 min or not classic\n",
      "Skipping: 211 due to <10 min or not classic\n",
      "Skipping: 212 due to <10 min or not classic\n",
      "Finished with: 213 of 249\n",
      "Finished with: 214 of 249\n",
      "Finished with: 215 of 249\n",
      "Finished with: 216 of 249\n",
      "Skipping: 217 due to <10 min or not classic\n",
      "Finished with: 218 of 249\n",
      "Finished with: 219 of 249\n",
      "Skipping: 220 due to <10 min or not classic\n",
      "Finished with: 221 of 249\n",
      "Skipping: 222 due to <10 min or not classic\n",
      "Finished with: 223 of 249\n",
      "Skipping: 224 due to <10 min or not classic\n",
      "Skipping: 225 due to <10 min or not classic\n",
      "Skipping: 226 due to <10 min or not classic\n",
      "Skipping: 227 due to <10 min or not classic\n",
      "Skipping: 228 due to <10 min or not classic\n",
      "Skipping: 229 due to <10 min or not classic\n",
      "Finished with: 230 of 249\n",
      "Finished with: 231 of 249\n",
      "Finished with: 232 of 249\n",
      "Skipping: 233 due to <10 min or not classic\n",
      "Finished with: 234 of 249\n",
      "Skipping: 235 due to <10 min or not classic\n",
      "Finished with: 236 of 249\n",
      "Finished with: 237 of 249\n",
      "Skipping: 238 due to <10 min or not classic\n",
      "Skipping: 239 due to <10 min or not classic\n",
      "Skipping: 240 due to <10 min or not classic\n",
      "Finished with: 241 of 249\n",
      "Finished with: 242 of 249\n",
      "Skipping: 243 due to <10 min or not classic\n",
      "Skipping: 244 due to <10 min or not classic\n",
      "Skipping: 245 due to <10 min or not classic\n",
      "Skipping: 246 due to <10 min or not classic\n",
      "Skipping: 247 due to <10 min or not classic\n",
      "Skipping: 248 due to <10 min or not classic\n",
      "Skipping: 249 due to <10 min or not classic\n",
      "Done! now lets get some korean bbq, more chicken plz! haha....follow suit\n",
      "Skipping: 0 due to <10 min or not classic\n",
      "Skipping: 1 due to <10 min or not classic\n",
      "Skipping: 2 due to <10 min or not classic\n",
      "Skipping: 3 due to <10 min or not classic\n",
      "Skipping: 4 due to <10 min or not classic\n",
      "Skipping: 5 due to <10 min or not classic\n",
      "Skipping: 6 due to <10 min or not classic\n",
      "Skipping: 7 due to <10 min or not classic\n",
      "Skipping: 8 due to <10 min or not classic\n",
      "Skipping: 9 due to <10 min or not classic\n",
      "Finished with: 10 of 249\n",
      "Skipping: 11 due to <10 min or not classic\n",
      "Skipping: 12 due to <10 min or not classic\n",
      "Finished with: 13 of 249\n",
      "Finished with: 14 of 249\n",
      "Finished with: 15 of 249\n",
      "Skipping: 16 due to <10 min or not classic\n",
      "Finished with: 17 of 249\n",
      "Skipping: 18 due to <10 min or not classic\n",
      "Finished with: 19 of 249\n",
      "Finished with: 20 of 249\n",
      "Skipping: 21 due to <10 min or not classic\n",
      "Skipping: 22 due to <10 min or not classic\n",
      "Skipping: 23 due to <10 min or not classic\n",
      "Skipping: 24 due to <10 min or not classic\n",
      "Skipping: 25 due to <10 min or not classic\n",
      "Skipping: 26 due to <10 min or not classic\n",
      "Finished with: 27 of 249\n",
      "Skipping: 28 due to <10 min or not classic\n",
      "Skipping: 29 due to <10 min or not classic\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a63110abe4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_extracted_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-cba72cf50342>\u001b[0m in \u001b[0;36mbuild_extracted_df\u001b[0;34m(username, path, time)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#Now feed them into Joshua C's prepare file. Extract data for the specified timeframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeline_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#Now append the temp_df to the complete_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/eSport-Analysis-League-of-Legends/chris/acquire.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(timeline_data, other_game_data, time)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Using timeline data to extract events, up to a specific time, and add them up to create totals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mkda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_player_kda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Saving totals (kda) to final_d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/eSport-Analysis-League-of-Legends/chris/acquire.py\u001b[0m in \u001b[0;36mget_player_kda\u001b[0;34m(data, time)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frames'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'events'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'WARD_PLACED'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mkills_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'WARD_PLACED'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   7944\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7945\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only append a dict if ignore_index=True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7946\u001b[0;31m                 \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7947\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7948\u001b[0m                 raise TypeError(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mibase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_extract_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mis_empty_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;31m# gh-17261\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 warnings.warn(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36mis_empty_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \"\"\"\n\u001b[1;32m    626\u001b[0m     \u001b[0mis_none\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0mis_list_like_without_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0mis_simple_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_list_like_without_dtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mis_none\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_simple_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "df = build_extracted_df(username, path = './', time = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28ad5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('min_10_start_3700_end_4173_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aed26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7e1e4",
   "metadata": {},
   "source": [
    "# Get the New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358fdbf1",
   "metadata": {},
   "source": [
    "Now that the function has been completed, get the desired data. Joshua C. is gathering the data for the pro players. I will get the data using the names from Jared's list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the names csv\n",
    "name_list = pd.read_csv('2640_players.csv', index_col = [0])\n",
    "name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c013d15",
   "metadata": {},
   "source": [
    "__How Many?__\n",
    "\n",
    "Since there are over 2600 names, I will only attempt to gather for the first 1000 of them. I will save a json file after every 250 json entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065680c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = list(name_list.players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bbffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = name_list[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function\n",
    "get_new_patch_data(name_list, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa02b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completed 320 names in name_list before it errored out.\n",
    "#Create new list starting at index 321 and restart the function\n",
    "name_list = name_list[321:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252864e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function\n",
    "get_new_patch_data(name_list, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51152e8",
   "metadata": {},
   "source": [
    "# Join All of the New Json Files Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea134e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import os and try to programatically grab the json file names\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ba20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_files = [ x for x in os.listdir('./new_data') if x.startswith(\"timeline\") ]\n",
    "timeline_files.sort()\n",
    "timeline_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a071d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data_files = [ x for x in os.listdir('./new_data') if x.startswith(\"other\") ]\n",
    "other_data_files.sort()\n",
    "other_data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afebafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ba1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to loop through these files and prepare them\n",
    "#Will need an empty df to store the final df\n",
    "complete_df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(timeline_files[:2])):\n",
    "    #Need to read both of them into dataframes and then convert to list of dicts\n",
    "    timeline_list = pd.read_json('./new_data/' + timeline_files[i])\n",
    "    game_list = pd.read_json('./new_data/' + other_data_files[i])\n",
    "\n",
    "    #Convert to list of dicts\n",
    "    timeline_list = timeline_list.to_dict(orient = 'records')\n",
    "    game_list = game_list.to_dict(orient = 'records')\n",
    "\n",
    "    #Now feed them into Joshua C's prepare file. Extract data at the 15 minute mark\n",
    "    temp_df = acquire.extract(timeline_list, game_list, 15)\n",
    "    print(f'Extracted the data! Entries: {len(temp_df)}')\n",
    "\n",
    "    #Now prepare the data\n",
    "    temp_df = prepare.prepare(temp_df)\n",
    "    print(f'Prepared the data! Entries: {len(temp_df)}')\n",
    "\n",
    "    #Now append the temp_df to the complete_df\n",
    "    complete_df = complete_df.append(temp_df, ignore_index = True)\n",
    "    print(f'Appended the data! Complete DataFrame Entries: {len(complete_df)}')\n",
    "\n",
    "#Drop duplicates and return the prepared df\n",
    "complete_df = complete_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffa02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77327caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the function\n",
    "def build_extracted_df(username, path = './', time = 15):\n",
    "    \"\"\"\n",
    "    This function will take in a path for the json files stored in your directory. \n",
    "    It will then read all of them into a list, convert them to lists of dicts, and feed\n",
    "    them into the extract function. The time variable is for the extract function and determines\n",
    "    at what timeframe the data will be acquired for. The username is only used for naming the\n",
    "    .csv file at the end. Please use your last name to avoid confusion.\n",
    "    \n",
    "    This function returns a dataframe with data extracted for the specified time. \n",
    "    It also automatically saves this dataframe as a .csv.\n",
    "    \n",
    "    The path variable defaults to the current directory.\n",
    "    The time variable defaults to the 15 minute mark.\n",
    "    \n",
    "    You will need to have os imported to run this function.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Gather the names of the timeline json files and sort them\n",
    "    #This will pull all files that start with 'timeline'\n",
    "    timeline_files = [ x for x in os.listdir(path) if x.startswith(\"timeline\") ]\n",
    "    timeline_files.sort()\n",
    "    \n",
    "    #Gather the names of the other game data json files and sort them\n",
    "    #This will pull all files that start with 'other'\n",
    "    other_data_files = [ x for x in os.listdir(path) if x.startswith(\"other\") ]\n",
    "    other_data_files.sort()\n",
    "    \n",
    "    #Will need an empty df to store the final df\n",
    "    extracted_df = pd.DataFrame()\n",
    "    \n",
    "    #Verify that the lists are the same length\n",
    "    if len(timeline_files) != len(other_data_files):\n",
    "        print('Something went wrong! The json lists are not equal!')\n",
    "        print('Stopping function.')\n",
    "        return\n",
    "\n",
    "    for i in range(0, len(timeline_files)):\n",
    "        #Need to read both of them into dataframes and then convert to list of dicts\n",
    "        timeline_list = pd.read_json(path + timeline_files[i])\n",
    "        game_list = pd.read_json(path + other_data_files[i])\n",
    "\n",
    "        #Convert to list of dicts\n",
    "        timeline_list = timeline_list.to_dict(orient = 'records')\n",
    "        game_list = game_list.to_dict(orient = 'records')\n",
    "\n",
    "        #Now feed them into Joshua C's prepare file. Extract data for the specified timeframe\n",
    "        temp_df = acquire.extract(timeline_list, game_list, time)\n",
    "\n",
    "        #Now append the temp_df to the complete_df\n",
    "        extracted_df = extracted_df.append(temp_df, ignore_index = True)\n",
    "            \n",
    "    #Drop duplicates and return the prepared df\n",
    "    extracted_df = extracted_df.drop_duplicates()\n",
    "    \n",
    "    #Now save the extracted_df as a .csv\n",
    "    extracted_df.to_csv(f'new_extracted_data_{username}.csv', index = False)\n",
    "    \n",
    "    return extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f66409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the function and build an extracted df that we can feed into the prepare function\n",
    "extracted_data = build_extracted_df(username = 'smith', path = './new_data/', time = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aed0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
